{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show()で可視化されない人はこのセルを実行してください。\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初めの一回だけこのセルを実行してください、データセットをダウンロードして展開します\n",
    "# 一回実行すれば、データセットはダウンロードされたままなので、再起動後等再び実行する必要はありません\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# URLを指定\n",
    "url = \"https://storage.googleapis.com/tutor-contents-dataset/5030_unsupervised_learning_data.zip\"\n",
    "save_name = url.split('/')[-1]\n",
    "\n",
    "# ダウンロードする\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# ファイルへ保存\n",
    "with open(save_name, mode='wb') as f:\n",
    "    f.write(mem)\n",
    "\n",
    "# zipファイルをカレントディレクトリに展開する\n",
    "zfile = zipfile.ZipFile(save_name)\n",
    "zfile.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "SkCXd0mZx-G",
    "id": "chapter_name"
   },
   "source": [
    "#  主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[3.1 主成分分析](#3.1-主成分分析)**\n",
    "    - **[3.1.1 主成分分析](#3.1.1-主成分分析)**\n",
    "    - **[3.1.2 特徴変換までの流れ](#3.1.2-特徴変換までの流れ)**\n",
    "    - **[3.1.3 データの用意](#3.1.3-データの用意)**\n",
    "    - **[3.1.4 標準化](#3.1.4-標準化)**\n",
    "    - **[3.1.5 相関行列の計算](#3.1.5-相関行列の計算)**\n",
    "    - **[3.1.6 固有値分解](#3.1.6-固有値分解)**\n",
    "    - **[3.1.7 特徴変換](#3.1.7-特徴変換)**\n",
    "    - **[3.1.8 scikit-learnを使った主成分分析](#3.1.8-scikit-learnを使った主成分分析)**\n",
    "    - **[3.1.9 前処理としての主成分分析](#3.1.9-前処理としての主成分分析)**\n",
    "<br><br>\n",
    "- **[3.2 カーネル主成分分析](#3.2-カーネル主成分分析)**\n",
    "    - **[3.2.1 カーネル主成分分析](#3.2.1-カーネル主成分分析)**\n",
    "    - **[3.2.2 カーネルトリックⅠ](#3.2.2-カーネルトリックⅠ)**\n",
    "    - **[3.2.3 カーネルトリックⅡ](#3.2.3-カーネルトリックⅡ)**\n",
    "    - **[3.2.4 特徴変換](#3.2.4-特徴変換)**\n",
    "    - **[3.2.5 scikit-learnを使ったカーネル主成分分析](#3.2.5-scikit-learnを使ったカーネル主成分分析)**\n",
    "<br><br>\n",
    "- **[3.3 まとめ問題(提出不要)](#3.3-まとめ問題(提出不要))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "HkyV_AQZgWz"
   },
   "source": [
    "## 3.1 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "HJMzc2Ui8ez",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.1 主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　<b style='color: #AA0000'>主成分分析</b>(Principal Component Analysis : <b style='color: #AA0000'>PCA</b>) は、**データの要約** （少ないデータ数で元のデータを表現すること）の強力な手法のひとつです。  \n",
    "\n",
    "　例として、主成分分析を用いて10人の学生の数学と国語の得点データ（2次元）を圧縮し1次元に変換するイメージを示します。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap3_3.png\">\n",
    "\n",
    "<b><center>図3.1.1-1 主成分分析のイメージ図</center></b>\n",
    "\n",
    "　右の図のように数学の得点だけ（1次元）で個々人の点数を説明するより、左の図のように新しい軸を1つ用意し新たに1次元データを作った方が小さい誤差で説明できます。  \n",
    "　左の図は主成分分析を用いたデータ圧縮の図です。**主成分分析**を用いると、**全部のデータを最も効率よく説明できるような軸（<b style='color: #AA0000'>第一主成分</b>の軸）**と**それだけでは説明しきれないデータを最も効率よく説明する軸（<b style='color: #AA0000'>第二主成分</b>の軸）**が作られます。  \n",
    "第一主成分は元のデータをよく表現できるので、第二主成分の情報を捨てる（使わない）ことで**効率よくデータの圧縮ができます**。\n",
    "\n",
    "　主成分分析の実用例として、製品やサービスのスコアリングや比較（1次元に圧縮）、データの可視化（2,3次元に圧縮）、回帰分析の前処理、などがあげられます。  \n",
    "**主成分分析は実用性が高く、機械学習の分野において重要なテーマの一つとなっています**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 主成分分析について述べた文として**正しいものを選んでください**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 主成分分析は、教師あり学習の一つの手法である。\n",
    "- 主成分分析を使う際、主成分となる軸を予め求めておく必要がある。\n",
    "- 主成分分析を用いると、効率よくデータの圧縮ができる。\n",
    "- 主成分分析を用いると必ずデータ数が減るため、実用的ではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 主成分分析は教師あり学習の前処理として使われることはありますが、主成分分析自体は教師あり学習に分類されません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 主成分分析を用いると、効率よくデータの圧縮ができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "HJeNdC7-gbM",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.2 特徴変換までの流れ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　主成分分析を使って、以下の手順で　<b style='color: #AA0000'>データの圧縮（特徴変換）</b>を行います。  \n",
    "　少し難しい内容になっていますが問題ありません。次のセッションから詳しく見ていきます。\n",
    "\n",
    "- データを**標準化**します。\n",
    "- 特徴同士の**相関行列**を計算します。\n",
    "- 相関行列の**固有ベクトルと固有値**を求めます。\n",
    "- 得られた固有値を大きい方からk個選び、対応する**固有ベクトルを選択**します。このkの値は、圧縮したい次元数を指定してください。\n",
    "- k個の固有ベクトルから**特徴変換行列** ${W}$ を作成します。\n",
    "- d次元のデータ ${X}$ と行列 ${W}$ で行列の積をとり、**k次元に変換されたデータ** ${X'}$ を得ます。\n",
    "\n",
    "　以下の図は、このチャプターで扱うワインのデータセットを <b style='color: #AA0000'>特徴変換</b>し、データを13次元から2次元に要約したイメージです。  \n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap3_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 主成分分析を用いた特徴変換について述べた文として、 *最も適切なもの* を選んでください。ただし、データ数（行数）をn,m、成分数（列数）をk,lとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- データ数n、成分数lのデータを特徴変換すると、データ数m（≠n）、成分数k（≠l）のデータができる。\n",
    "- データ数n、成分数lのデータを特徴変換すると、データ数n、成分数k（≠l）のデータができる。\n",
    "- データ数n、成分数lのデータを特徴変換すると、データ数m（≠n）、成分数1のデータができる。\n",
    "- データ数n、成分数lのデータを特徴変換すると、データ数n、成分数1のデータができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 主成分分析を適切に適応させることにより、特徴量を欲しい次元に要約したデータを得ることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- データ数n、成分数lのデータを特徴変換すると、データ数n、成分数k（≠l）のデータができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "ry7z9nIjUgM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.3 データの用意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　実際に**主成分分析**を行っていきましょう。用いるデータは、　**「UCI Machine Learning Repository」**　で公開されているワインのデータです。　**178行**分のワインサンプルに対して、**ぶどうの種類データ**（1〜3のラベル）と**ワインの化学的性質を表す特徴量データ**（13種類）で構成されています。\n",
    "\n",
    "　以下のようにしてデータを取得します。\n",
    "```python\n",
    "import pandas as pd\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header = None)\n",
    "# 特徴量データをXに、ラベルデータをyに格納。\n",
    "# df_wineの1列目はラベルデータ、2列目以降は特徴量データです。\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めてワインのデータを用意し、 **データ数と各データの特徴量の数を取得、表示** してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# ここに回答を記述してください\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "`X`, `y` はnumpy配列です。  \n",
    "numpy配列 `A` のサイズは以下のようにして取得、表示できます。\n",
    "```python\n",
    "print(A.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# ここに回答を記述してください\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "SJ-EuRQ-l-f",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.4 標準化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "予めワインのデータを、各特徴量について平均が0、分散が1となるように変換します。これを <b style='color: #AA0000'>標準化</b>と言います。  \n",
    "　<b style='color: #AA0000'>標準化</b>を行うことによって、アルコール度数やワインの色相など、単位も基準となる値もバラバラな **様々な種類のデータを同じように扱うことが可能になります**。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "# 標準化\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めて **標準化** を行ってください。また結果を見て **次の2点を確認してください。**\n",
    " - 標準化によって平均がほぼ0に分散が1になっており、かつ分布の形は変わっていないこと。\n",
    " - 特徴量同士の関連性が明らかに高そうなものがあること。（今後、関連性の高いデータ同士をまとめることによってデータを圧縮していきます。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 標準化前のデータを可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.set_title('before')\n",
    "ax2.set_title('before')\n",
    "ax1.scatter(X[:, 0], X[:, 1])\n",
    "ax2.scatter(X[:, 5], X[:, 6])\n",
    "plt.show()\n",
    "\n",
    "print(\"before\")\n",
    "print(\"mean: \", X.mean(axis=0), \"\\nstd: \", X.std(axis=0))\n",
    "\n",
    "# Xに、Xを標準化したデータを代入してください\n",
    "X =\n",
    "\n",
    "# 標準化後のデータを可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.set_title('after')\n",
    "ax2.set_title('after')\n",
    "ax1.scatter(X[:, 0], X[:, 1])\n",
    "ax2.scatter(X[:, 5], X[:, 6])\n",
    "plt.show()\n",
    "\n",
    "print(\"after\")\n",
    "print(\"mean: \", X.mean(axis=0), \"\\nstd: \", X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "`X` はnumpy配列です。  \n",
    "`X`を標準化するには以下のようにします。\n",
    "```python\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 標準化前のデータを可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.set_title('before')\n",
    "ax2.set_title('before')\n",
    "ax1.scatter(X[:, 0], X[:, 1])\n",
    "ax2.scatter(X[:, 5], X[:, 6])\n",
    "plt.show()\n",
    "\n",
    "print(\"before\")\n",
    "print(\"mean: \", X.mean(axis=0), \"\\nstd: \", X.std(axis=0))\n",
    "\n",
    "# Xに、Xを標準化したデータを代入してください\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# 標準化後のデータを可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.set_title('after')\n",
    "ax2.set_title('after')\n",
    "ax1.scatter(X[:, 0], X[:, 1])\n",
    "ax2.scatter(X[:, 5], X[:, 6])\n",
    "plt.show()\n",
    "\n",
    "print(\"after\")\n",
    "print(\"mean: \", X.mean(axis=0), \"\\nstd: \", X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "H1fEuCm-lZG",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.5 相関行列の計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　<b>特徴量ごとの類似度</b>を調べるため、データの <b style='color: #AA0000'>相関行列</b>を計算します。  \n",
    "\n",
    "　<b>相関係数</b>は、2つのデータ間の<b>直線的な関係性の強さ</b>を表す指標で -1〜1の値をとります。  \n",
    "　<b>相関係数が1に近い</b>（正の相関が強い）ときは、左の図のように<b>二つのデータは片方が増加するともう片方も増加する</b> ような一次関数的な分布をとります。<br>\n",
    "　<b>負の相関が強い</b> ときは <b>片方が増加するともう片方は減少する</b> ような一次関数的な分布をとります。<br>\n",
    "　<b>相関係数が0に近い</b> ときは、右の図のように <b>直線的な関係性があまり見られません。</b>\n",
    " \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap3_1.png\" width=500>\n",
    " \n",
    "　ここでは、ワインの13種類の特徴データについて、それぞれの相関係数を保持する13x13の相関行列を求めます。求める <b>相関行列</b> は以下のような形をしています。\n",
    "\n",
    "$$\n",
    "R = \n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      r_{1,1} & r_{1,2} & \\cdots & r_{1,13} \\\\\n",
    "      r_{2,1} & r_{2,2} & \\cdots & r_{2,13} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      r_{13,1} & r_{2,13} & \\cdots & r_{13,13} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "　以下のようにして <b>相関行列</b> を取得します。 corrcoef()関数は、列同士(横方向)のそれぞれの相関ではなく、行同士(縦方向)のそれぞれの相関を相関行列にします。そのため、このままではデータ同士の相関行列が求まってしまいます。<b>データ同士ではなく特徴同士の相関行列を求めるため</b>、X.Tで${X}$ を転置しています。\n",
    "```python\n",
    "import numpy as np\n",
    "R = np.corrcoef(X.T)\n",
    "```\n",
    "　発展的な話になりますが、相関行列自体は標準化前の ${X}$ を用いても同様に計算できますが、この後のために予め標準化しました。  \n",
    "　また、相関行列ではなく共分散行列を使った主成分分析もあります。\n",
    " \n",
    "正方行列の単位行列を作成するコードは以下になります。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# np.identity(行列サイズ)\n",
    "identity　= np.identity(3)\n",
    "\n",
    "array([[1., 0., 0.],\n",
    "       [0., 1., 0.],\n",
    "       [0., 0., 1.]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めて相関行列を求めてください。\n",
    "- また、最も相関の高い特徴の組み合わせを調べ、コード中のコメント文に従ってその相関係数を出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 相関行列（13x13）を作成\n",
    "R = np.corrcoef(X.T)\n",
    "\n",
    "# 対角成分を0にしてください\n",
    "_R =\n",
    "\n",
    "# 最大相関係数をとるインデックスを1つだけ取得してください\n",
    "index =\n",
    "\n",
    "print(R[index[0], index[1]])\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "`R` はndarrayです。  \n",
    "ndarray配列 `A` の最大要素値のインデックスを全て取得、表示するには以下のようにします。\n",
    "```python\n",
    "print(np.where(A == A.max()))\n",
    "```\n",
    "なお、対角成分は1（つまり、 `R[i,i] == 1` ）ですが、これらは相関係数ではないので注意してください。  \n",
    "例えば、以下の式で相関行列 `R` の対角成分を0にした `_R` を生成できます。\n",
    "```python\n",
    "_R = R - np.identity(13)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 相関行列（13x13）を作成\n",
    "R = np.corrcoef(X.T)\n",
    "\n",
    "# 対角成分を0にしてください\n",
    "_R = R - np.identity(13)\n",
    "\n",
    "# 最大相関係数をとるインデックスを1つだけ取得してください\n",
    "index = np.where(_R == _R.max())[0]\n",
    "\n",
    "print(R[index[0], index[1]])\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "H1QE_AXWeZG",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.6 固有値分解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　　次に、得られた相関行列に <b style='color: #AA0000'>固有値分解</b>という数学的手法を適用し、固有ベクトルと固有値を取得します。  \n",
    "　固有値分解を行うと、もとの13x13次元の行列 ${R}$ は13個の特別な13次元ベクトル **（固有ベクトル）** $v_1$ 〜 $v_{13}$ と13個の特別な数 **（固有値）** $\\lambda_1$ 〜 $\\lambda_{13}$ に分解されます。直感的には、元の行列は **固有ベクトルの方向に情報が集中しており、対応する固有値は情報の集中の度合いを示している** と言えます。\n",
    "\n",
    "\n",
    "　numpyを使って以下のように固有値分解を計算できます。 ${R}$ の固有値13個と固有ベクトル13個がそれぞれ、 `eigvals` と `eigvecs` に格納されます。\n",
    "```python\n",
    "import numpy as np\n",
    "# 相関行列から固有対を取得。 numpy.linalg.eighはそれらを固有値の昇順で返す\n",
    "eigvals, eigvecs = np.linalg.eigh(R)\n",
    "```\n",
    "\n",
    "　以下は発展的内容になるので読み飛ばしても構いません。  \n",
    "\n",
    "　相関行列 ${R}$ 、固有ベクトルを並べた行列 ${V}$ と固有値を並べた対角行列 ${D}$ は以下の式を満たします。\n",
    "\n",
    "$${R}{V} = {V}{D}$$\n",
    "\n",
    "　要素を表すと以下のようになります。\n",
    "\n",
    "$$\n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      r_{1,1} & r_{1,2} & \\cdots & r_{1,13} \\\\\n",
    "      r_{2,1} & r_{2,2} & \\cdots & r_{2,13} \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      r_{13,1} & r_{2,13} & \\cdots & r_{13,13} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      & & &  \\\\\n",
    "      & & &  \\\\\n",
    "      { v_{1}} & { v_{2}} & \\cdots & { v_{13}} \\\\\n",
    "      & & &  \\\\\n",
    "      & & &  \\\\\n",
    "    \\end{array}\n",
    "  \\right) = \n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      & & &  \\\\\n",
    "      & & &  \\\\\n",
    "      { v_{1}} & { v_{2}} & \\cdots & { v_{13}} \\\\\n",
    "      & & &  \\\\\n",
    "      & & &  \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      \\lambda_{1} & 0 & \\cdots & 0 \\\\\n",
    "      0 & \\lambda_{2} & \\cdots & 0 \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      0 & \\cdots & 0 & \\lambda_{13} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$\n",
    "\n",
    "　相関行列の固有ベクトルが **主成分ベクトル** を表し、固有ベクトルの成分は **各特徴量の主成分への影響の様子** を表しています。また、 **大きな固有値に対応する固有ベクトルほど元の行列の構成に深く関わっています** 。  \n",
    "　つまり、小さい固有値に対応する固有ベクトル（主成分ベクトル）を無視することで、特徴量を削減しつつ情報の損失を少なく抑えることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めて固有値分解を行ってください。また出力される図を見て、固有値に大きな偏りがあることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 相関行列（13x13）を作成\n",
    "R = np.corrcoef(X.T)\n",
    "\n",
    "# 固有値分解\n",
    "eigvals, eigvecs =\n",
    "\n",
    "# 可視化\n",
    "plt.bar(range(13), eigvals)\n",
    "plt.title(\"distribution of eigvals\")\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"eigvals\")\n",
    "plt.show()\n",
    "\n",
    "# 消さないでください。実行結果の確認に使います。\n",
    "print(eigvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `numpy.linalg.eigh` は、小さい固有値から順に格納されたものをひとつ目の返り値として出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 相関行列（13x13）を作成\n",
    "R = np.corrcoef(X.T)\n",
    "\n",
    "# 固有値分解\n",
    "eigvals, eigvecs = np.linalg.eigh(R)\n",
    "\n",
    "# 可視化\n",
    "plt.bar(range(13), eigvals)\n",
    "plt.title(\"distribution of eigvals\")\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"eigvals\")\n",
    "plt.show()\n",
    "\n",
    "# 消さないでください。実行結果の確認に使います。\n",
    "print(eigvals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "BkV4ORQWlbM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.7 特徴変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　前のセッションでは、相関行列を固有値と固有ベクトルに分解しました。次に、その中から最大の固有値と2番目に大きい固有値に対応する2つの固有ベクトルを用いて、<b>13次元の特徴量を2次元に変換する</b>13x2の行列 ${W}$ を作り、13次元特徴量を持つWineデータ ${X}$ を、<b>第一主成分</b>と<b>第二主成分</b>の2次元の特徴量のみを持つ<b>新たなWineデータ</b> ${X'}$ に変換します。<br>\n",
    " \n",
    "　以下のようにして変換行列 ${W}$ を作ります。\n",
    " \n",
    "```python\n",
    "# 列方向で、固有値が最大のものと2番目に最大のものに対応する固有ベクトルを連結します\n",
    "W = np.c_[eigvecs[:,-1], eigvecs[:,-2]]\n",
    "```\n",
    "\n",
    "　上記より、13行2列の行列が作成できました。さらに、この行列 ${W}$ と元のデータ${X}$ との積を取ることにより、 ${X}$ を圧縮した行列 ${X'}$ を生成できます。\n",
    "\n",
    "$${X'} = {X}{W}$$\n",
    "                    \n",
    "　行列の積の計算は、以下のコードで実行されます。\n",
    "```python\n",
    "import numpy as np\n",
    "X_pca = X.dot(W)\n",
    "```\n",
    "\n",
    "　以下は発展的な内容になるため読み飛ばして構いません。  \n",
    "\n",
    "　固有ベクトルと固有値を用いて、以下の式も成り立ちます。  \n",
    "\n",
    "$${ R}{ V'} = { V'}{ D'}$$\n",
    "\n",
    "$$\n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      r_{1,1} & r_{1,2} & \\cdots & r_{1,13} \\\\\n",
    "      r_{2,1} & r_{2,2} & \\cdots & r_{2,13} \\\\\n",
    "      \\vdots & \\vdots& \\ddots& \\vdots \\\\\n",
    "      r_{13,1} & r_{13,2} & \\cdots & r_{13,13} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      & \\\\\n",
    "      & \\\\\n",
    "      { v_{1}} & { v_{2}} \\\\\n",
    "      & \\\\\n",
    "      & \\\\\n",
    "    \\end{array}\n",
    "  \\right) = \n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      & \\\\\n",
    "      & \\\\\n",
    "      { v_{1}} & { v_{2}} \\\\\n",
    "      & \\\\\n",
    "      & \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      \\lambda_{1} & 0 \\\\\n",
    "      0 & \\lambda_{2} \\\\\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$  \n",
    "  \n",
    "　大きい方から2つの固有値を $\\lambda_1, \\lambda_2$ 、それに対応する固有ベクトルを ${v_1}, {v_2}$ とします。この ${v_1}, {v_2}$ が変換行列となります。  \n",
    "　${R}$ に固有ベクトル ${v_1}$ を掛けると、 ${v_1}$ <b>方向によく伸びた（分散の大きい）新たなデータ</b>が得られ、${R}$ に固有ベクトル ${v_2}$ を掛けると、 ${v_1}$ <b>ベクトルとは直行する（</b> ${v_1}$ <b>では説明できない）、</b> ${v_2}$ <b>方向によく伸びた（分散の大きい）新たなデータ</b>が得られます。  \n",
    "　同様に、 ${X}$ に 固有ベクトル ${v_1}, {v_2}$ を掛けると、<b>2つの直行する方向によく伸びた特徴データ</b>を得ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めて、2次元に縮約されたデータを確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 標準化\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# 相関行列の取得\n",
    "R = np.corrcoef(X.T)\n",
    "\n",
    "# 固有値分解\n",
    "eigvals, eigvecs = np.linalg.eigh(R)\n",
    "\n",
    "# 変換行列の取得\n",
    "W =\n",
    "\n",
    "# 特徴変換\n",
    "X_pca =\n",
    "\n",
    "# 可視化\n",
    "color = [\"r\", \"b\", \"g\"]\n",
    "marker = [\"s\", \"x\", \"o\"]\n",
    "for label, color, marker in zip(np.unique(y), color, marker):\n",
    "    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1],\n",
    "                c=color, marker=marker, label=label)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "\n",
    "print(X_pca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 13次元のデータを2次元に圧縮したので、13次元のデータとは違い簡単に可視化できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "\n",
    "# 標準化\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# 相関行列の取得\n",
    "R = np.corrcoef(X.T)\n",
    "\n",
    "# 固有値分解\n",
    "eigvals, eigvecs = np.linalg.eigh(R)\n",
    "\n",
    "# 変換行列の取得\n",
    "W = np.c_[eigvecs[:, -1], eigvecs[:, -2]]\n",
    "\n",
    "# 特徴変換\n",
    "X_pca = X.dot(W)\n",
    "\n",
    "# 可視化\n",
    "color = [\"r\", \"b\", \"g\"]\n",
    "marker = [\"s\", \"x\", \"o\"]\n",
    "for label, color, marker in zip(np.unique(y), color, marker):\n",
    "    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1],\n",
    "                c=color, marker=marker, label=label)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "\n",
    "print(X_pca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "rJHf53IjUez",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.8 scikit-learnを使った主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　ここまで、一つずつステップを踏んで主成分分析を用いた特徴変換を実装しました。実は同様のことを、 `sklearn.decomposition` の `PCA` クラスを用いて簡単にできます。  \n",
    "　以下のようにして**PCAクラス**を使います。\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "# 主成分数を指定して、PCAのインスタンスを生成。引数で変換後の次元数を指定します。\n",
    "pca = PCA(n_components=2)\n",
    "# データから変換モデルを学習し、変換する。\n",
    "X_pca = pca.fit_transform(X)\n",
    "```\n",
    "　`fit_transform()` メソッドでは、内部で自動的に変換行列を生成しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めてPCAクラスを用いて特徴変換を行い、2次元に縮約されたデータを確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCAをインポートしてください\n",
    "# ---------------------------\n",
    "\n",
    "# ---------------------------\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# 主成分分析のインスタンスを生成。主成分数は2としてください。\n",
    "pca =\n",
    "\n",
    "# データから変換モデルを学習し、変換する。\n",
    "X_pca =\n",
    "\n",
    "# 可視化\n",
    "color = [\"r\", \"b\", \"g\"]\n",
    "marker = [\"s\", \"x\", \"o\"]\n",
    "for label, color, marker in zip(np.unique(y), color, marker):\n",
    "    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1],\n",
    "                c=color, marker=marker, label=label)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "\n",
    "print(X_pca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 細かい実装の違いによって、前セッションと比べると上下や左右に反転されたようにプロットされることがありますが、本質的に同じような特徴変換が行われていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PCAをインポートしてください\n",
    "# ---------------------------\n",
    "from sklearn.decomposition import PCA\n",
    "# ---------------------------\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# 主成分分析のインスタンスを生成\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# データから変換モデルを学習し、変換する。\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# 可視化\n",
    "color = [\"r\", \"b\", \"g\"]\n",
    "marker = [\"s\", \"x\", \"o\"]\n",
    "for label, color, marker in zip(np.unique(y), color, marker):\n",
    "    plt.scatter(X_pca[y == label, 0], X_pca[y == label, 1],\n",
    "                c=color, marker=marker, label=label)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()\n",
    "\n",
    "print(X_pca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "rkrEu0X-gZz",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.1.9 前処理としての主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　ここまで学んだことを応用して、**回帰分析の前処理に主成分分析を適用します**。予めデータを圧縮することで、外れ値などの外乱に強く**より汎用性の高い回帰分析モデルを生成できます**。  \n",
    "\n",
    "　まず、データをトレーニングデータとテストデータに分割します。\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n",
    "```\n",
    "\n",
    "　特徴変換を行う際、トレーニングデータとテストデータで違う変換行列を求めて特徴変換を行ってしまうと、変換行列が異なってしまうために特徴変換後のデータを比較することができません。標準化についても同じことが言えます。  \n",
    "　これでは不便なことがあるので、**標準化と主成分分析を行う際はトレーニングデータとテストデータで共通の基準を使います**。\n",
    "\n",
    "　**標準化**する際は、以下のように `StandardScalar` クラスを用いると便利です。\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 標準化のためのインスタンスを生成\n",
    "sc = StandardScaler()\n",
    "# トレーニングデータから変換モデルを学習し、テストデータに同じモデルを適用\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "```\n",
    "\n",
    "　**主成分分析**する際は、 `PCA` クラスを以下のように使います。\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "# 主成分分析のインスタンスを生成\n",
    "pca = PCA(n_components=2)\n",
    "# トレーニングデータから変換モデルを学習し、テストデータに同じモデルを適用\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "```\n",
    "\n",
    "復習となりますが、回帰分析は以下のように行います。\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# ロジスティック回帰のインスタンスを生成\n",
    "lr = LogisticRegression()\n",
    "# 分類モデルを学習\n",
    "lr.fit(X, y)\n",
    "# スコアの表示\n",
    "print(lr.score(X, y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めてWineデータを2次元のデータに圧縮したあと、ロジスティック回帰を使用して分類しましょう。\n",
    "- 次元を13から2に減らしたにも関わらず高い分類精度を出せていることを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# 標準化のためのインスタンスを生成\n",
    "sc =\n",
    "# トレーニングデータから変換モデルを学習し、テストデータに適用\n",
    "X_train_std =\n",
    "X_test_std =\n",
    "\n",
    "# 主成分分析のインスタンスを生成\n",
    "pca =\n",
    "# トレーニングデータから変換モデルを学習し、テストデータに適用\n",
    "X_train_pca =\n",
    "X_test_pca =\n",
    "\n",
    "# ロジスティック回帰のインスタンスを生成\n",
    "lr = LogisticRegression()\n",
    "# 次元削減後のトレーニングデータで分類モデルを学習\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "# スコアの表示\n",
    "print(lr.score(X_train_pca, y_train))\n",
    "print(lr.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 次元削除したデータをモデルの学習に使うと、若干ではありますが一般的にモデルの学習率が低下してしまいます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "df_wine = pd.read_csv(\"./5030_unsupervised_learning_data/wine.csv\", header=None)\n",
    "\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=0)\n",
    "\n",
    "# 標準化のためのインスタンスを生成\n",
    "sc = StandardScaler()\n",
    "# トレーニングデータから変換モデルを学習し、テストデータに適用\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "# 主成分分析のインスタンスを生成\n",
    "pca = PCA(n_components=2)\n",
    "# トレーニングデータから変換モデルを学習し、テストデータに適用\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "# ロジスティック回帰のインスタンスを生成\n",
    "lr = LogisticRegression()\n",
    "# 次元削減後のトレーニングデータで分類モデルを学習\n",
    "lr.fit(X_train_pca, y_train)\n",
    "\n",
    "# スコアの表示\n",
    "print(lr.score(X_train_pca, y_train))\n",
    "print(lr.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "ryINORQWgWM"
   },
   "source": [
    "## 3.2 カーネル主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "BkPNuAmbebG",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.1 カーネル主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　回帰分析など、機械学習の多くのアルゴリズムは線形分離できるデータが与えられることを前提としています。ただ、現実問題として線形分離することが困難なデータ、つまり**非線形分離する必要があるデータがほとんどです**。このセッションでは、非線形分離する必要があるデータに対処できるカーネル化されたPCA、 **「カーネルPCA(kernel PCA)」** について本項では取り上げます。\n",
    "\n",
    "　**カーネルPCA** ではまずはじめに、与えられた$N$x$M$（データ数x特徴の種類）のデータ ${X}$ を、**全く新しい**$N$x$M´$（データ数x特徴の種類）**のデータ** ${K}$ **に作り変えます <b style='color: #AA0000'>（カーネルトリック）</b>。** **カーネルトリック**を用いると一般的に特徴の種類は多くなり（特徴量が展開され）、**線形分離させやすくなります**。非線形性の高いデータに対して主成分分析を用いてもうまく行かないことが知られていますが、データをカーネル行列 ${K}$ に展開することで主成分分析ができるようになります。\n",
    " \n",
    "　下の図は、円形に分布する2次元のデータに、カーネルトリックを使い特徴量を増やした後主成分分析を行って特徴量を2つに戻してプロットした図です。カーネルPCAを用いると、以下のような2次元空間で**線形分離不可能なデータ**を**線形分離可能なデータに変換する**ことができます。\n",
    " \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap3_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- カーネル主成分分析について述べた文として最も適切なものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- カーネル主成分分析は、特徴量の次元を1次元に圧縮することを目的としている。\n",
    "- カーネル主成分分析は、特徴量の次元を無限次元に展開することを目的としている。\n",
    "- カーネル主成分分析は、線形分離可能な特徴量を線形分離不可能なものに変換することを目的としている。\n",
    "- カーネル主成分分析は、線形分離不可能な特徴量を線形分離可能なものに変換することを目的としている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- カーネル主成分分析を適切に適応させることにより、欲しい次元分の線形分離可能な特徴量に要約したデータを得ることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- カーネル主成分分析は、線形分離不可能な特徴量を線形分離可能なものに変換することを目的としている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "H1O4uA7ZgWM",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.2 カーネルトリックⅠ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　まず <b style='color: #AA0000'>カーネル(類似度)行列</b>${K}$ を計算します。以下のような行列を**カーネル行列**といい、**全てのサンプルデータのペアごとに類似度を計算しています**。$N$x$M$（データ数x特徴の種類）のデータ ${X}$ のカーネル行列は、$N$x$N$ （データ数xデータ数）になります。**カーネル行列** ${K}$ **を、** ${X}$ **に代わる新たな**$N$x$M$**（データ数x特徴の種類）のデータ**と見なすことができます。\n",
    "\n",
    "$${K} = \n",
    "  \\left(\n",
    "    \\begin{array}{ccc}\n",
    "      k(\\vec{x}_{1},\\vec{x}_{1}) & k(\\vec{x}_{1},\\vec{x}_{2}) & \\cdots & k(\\vec{x}_{1},\\vec{x}_{n}) \\\\\n",
    "      k(\\vec{x}_{2},\\vec{x}_{1}) & k(\\vec{x}_{2},\\vec{x}_{2}) & \\cdots & k(\\vec{x}_{2},\\vec{x}_{n}) \\\\\n",
    "      \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "      k(\\vec{x}_{n},\\vec{x}_{1}) & k(\\vec{x}_{n},\\vec{x}_{2}) & \\cdots & k(\\vec{x}_{n},\\vec{x}_{n})\n",
    "    \\end{array}\n",
    "  \\right)\n",
    "$$\n",
    "\n",
    "　ここで示されている$k(\\vec{x}_{i}, \\vec{x}_{j})$は、 **「カーネル関数」** と呼ばれ、数種類存在します。今回は、 **「動径基底関数 (RBF)」** または **「ガウスカーネル」** と呼ばれ以下の式で表されるカーネル関数を使います。この式で2つのデータ $\\vec{x}_{i},\\vec{x}_{j}$ の類似度を表しています。\n",
    " \n",
    "$$k(\\vec{x}_{i},\\vec{x}_{j}) \\equiv \\mathrm{exp}(-\\gamma |\\vec{x}_{i}-\\vec{x}_{j}|^2)$$\n",
    "\n",
    "　 ${K}$ は ${X}$ と同じように、行が個々のデータを、列が各特徴量（他のデータとの類似度）を示します。  \n",
    "            \n",
    "　ここで、$y =  \\mathrm{exp}(-\\gamma x^2)$ は以下のようなグラフの関数です。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap3_5.png\">\n",
    "\n",
    "　$\\gamma$ を大きくすると、 **より近接するものだけに注目したような特徴量行列** ${K}$ **が作られます** 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- カーネルトリックについて述べた文として最も適切なものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- データ数n、特徴数dのデータにカーネルトリックを用いると、データ数n、特徴数dのデータができる。\n",
    "- データ数n、特徴数dのデータにカーネルトリックを用いると、データ数m（≠n）、特徴数k（≠d）のデータができる。\n",
    "- データ数n、特徴数dのデータにカーネルトリックを用いると、データ数n、特徴数nのデータができる。\n",
    "- データ数n、特徴数dのデータにカーネルトリックを用いると、データ数d、特徴数dのデータができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- カーネルトリックによって、各データの特徴量数はデータの数と一致するようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- データ数n、特徴数dのデータにカーネルトリックを用いると、データ数n、特徴数nのデータができる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "S18fq3IoLez",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.3 カーネルトリックⅡ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　**カーネルトリック**に用いる以下の関数を実装します。  \n",
    "\n",
    "$$k(\\vec{x}_{i},\\vec{x}_{j}) \\equiv \\mathrm{exp}(-\\gamma |\\vec{x}_{i}-\\vec{x}_{j}|^2)$$\n",
    "\n",
    "　以下のようにして**カーネル行列**を計算できます。\n",
    "```python\n",
    "# データ同士の距離の2乗（平方ユークリッド距離）を計算\n",
    "M = np.sum((X - X[:, np.newaxis])**2, axis=2)\n",
    "# カーネル行列を計算\n",
    "K = np.exp(-gamma * M)\n",
    "```\n",
    "　ここでデータ間の距離の取得には、numpy配列のブロードキャストという機能（自動的に行列を展開して行列の形を揃え、演算を実行する）を使っています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めて$X$のカーネル行列を作り、カーネル行列の大きさを取得、出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(39)\n",
    "\n",
    "X = np.random.rand(8, 3)\n",
    "\n",
    "# ペアごとの平方ユークリッド距離を計算\n",
    "M =\n",
    "\n",
    "# カーネル行列を計算\n",
    "gamma = 15\n",
    "K =\n",
    "\n",
    "# ---------------------------\n",
    "#        ここを書いて下さい\n",
    "# ---------------------------\n",
    "\n",
    "print(M)  # 消さないでください。実行結果の確認に使います。\n",
    "print(K)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "K はnumpy配列です。  \n",
    "numpy配列 A のサイズは以下のようにして取得、表示できます。\n",
    "```python\n",
    "print(A.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(39)\n",
    "\n",
    "X = np.random.rand(8, 3)\n",
    "\n",
    "# ペアごとの平方ユークリッド距離を計算\n",
    "M = np.sum((X - X[:, np.newaxis])**2, axis=2)\n",
    "\n",
    "# カーネル行列を計算\n",
    "gamma = 15\n",
    "K = np.exp(-gamma * M)\n",
    "\n",
    "# ---------------------------\n",
    "print(K.shape)\n",
    "# ---------------------------\n",
    "\n",
    "print(M)  # 消さないでください。実行結果の確認に使います。\n",
    "print(K)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "BkKVuR7-g-M",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.4 特徴変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "カーネル行列 ${K}$ を元のデータ ${X}$ の代わりとし、 ${K}$ について<b>標準的な主成分分析手法と同様に</b>固有値分解、特徴変換などを行うと、<b>線形分離可能</b>なデータ<b>${X'}$</b>に変換できます。もともと ${K}$ は ${X}$ の特徴量を展開したものですから、 ${K}$ を特徴変換して得られる行列は、 ${X}$ の特徴量を変換した行列として扱うことができます。\n",
    "\n",
    "これまで学んだことを使って、円状のデータをカーネル主成分分析を用いて特徴変換しましょう。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap3_4.png\">\n",
    "\n",
    "<b><center>図 3.2.4 特徴変換</center></b>\n",
    "\n",
    "　発展的な話になりますが、カーネル行列 ${K}$ の固有ベクトルから作る変換行列 ${W}$ は、${X}$ を次元圧縮し要約した ${X'}$ としてそのまま扱うこともできます。ですがここでは先に学んだ主成分分析での手法に沿って特徴変換を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- カーネル主成分分析を用いて、円状のデータを線形分離可能な形に変換してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# データが円状に分布するデータを取得\n",
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "# ペアごとの平方ユークリッド距離を計算してください。\n",
    "M =\n",
    "\n",
    "# 対称カーネル行列を計算。γの値は15にしてください。\n",
    "gamma = \n",
    "K =\n",
    "\n",
    "# カーネル行列から固有対を取得。 numpy.linalg.eighはそれらを固有値の昇順で返す\n",
    "eigvals, eigvecs = np.linalg.eigh(K)\n",
    "# 上位k個の固有ベクトル(射影されたサンプル)を収集\n",
    "W = np.column_stack((eigvecs[:, -1], eigvecs[:, -2]))\n",
    "\n",
    "# KとWの内積を求めて線形分離可能なデータを獲得してください。\n",
    "X_kpca =\n",
    "\n",
    "# 可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.scatter(X[y == 0, 0], X[y == 0, 1], color=\"r\", marker=\"^\")\n",
    "ax1.scatter(X[y == 1, 0], X[y == 1, 1], color=\"b\", marker=\"o\")\n",
    "ax2.scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], color=\"r\", marker=\"^\")\n",
    "ax2.scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], color=\"b\", marker=\"o\")\n",
    "ax1.set_title(\"circle_data\")\n",
    "ax2.set_title(\"kernel_pca\")\n",
    "plt.show()\n",
    "\n",
    "print(M)  # 消さないでください。実行結果の確認に使います。\n",
    "print(K)  # 消さないでください。実行結果の確認に使います。\n",
    "print(W)  # 消さないでください。実行結果の確認に使います。\n",
    "print(X_kpca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- カーネル行列を作った後、主成分分析の項で学んだのと同じようにカーネル行列の固有値分解を行い、変換行列を求め、カーネル行列に変換行列を掛けあわせることで線形分離可能なデータに変換できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# データが円状に分布するデータを取得\n",
    "X, y = make_circles(n_samples=1000, random_state=123, noise=0.1, factor=0.2)\n",
    "\n",
    "# ペアごとの平方ユークリッド距離を計算\n",
    "M = np.sum((X - X[:, np.newaxis])**2, axis=2)\n",
    "\n",
    "# 対称カーネル行列を計算\n",
    "gamma = 15\n",
    "K = np.exp(-gamma * M)\n",
    "\n",
    "# カーネル行列から固有対を取得。 numpy.linalg.eighはそれらを固有値の昇順で返す\n",
    "eigvals, eigvecs = np.linalg.eigh(K)\n",
    "# 上位k個の固有ベクトル(射影されたサンプル)を収集\n",
    "W = np.column_stack((eigvecs[:, -1], eigvecs[:, -2]))\n",
    "\n",
    "# KとWの内積を求めて線形分離可能なデータを獲得してください。\n",
    "X_kpca = K.dot(W)\n",
    "\n",
    "# 可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.scatter(X[y == 0, 0], X[y == 0, 1], color=\"r\", marker=\"^\")\n",
    "ax1.scatter(X[y == 1, 0], X[y == 1, 1], color=\"b\", marker=\"o\")\n",
    "ax2.scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], color=\"r\", marker=\"^\")\n",
    "ax2.scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], color=\"b\", marker=\"o\")\n",
    "ax1.set_title(\"circle_data\")\n",
    "ax2.set_title(\"kernel_pca\")\n",
    "plt.show()\n",
    "\n",
    "print(M)  # 消さないでください。実行結果の確認に使います。\n",
    "print(K)  # 消さないでください。実行結果の確認に使います。\n",
    "print(W)  # 消さないでください。実行結果の確認に使います。\n",
    "print(X_kpca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "r1PGq38sIlM",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 3.2.5 scikit-learnを使ったカーネル主成分分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　**カーネル主成分分析**は、標準のPCAと同じように `sklearn.decomposition` を使って簡単に実装できます。使い方はほとんど標準のPCAと同様です。引数で圧縮後の次元の数と、標準のPCAにはなかったカーネルの種類を指定することができます。\n",
    "　\n",
    "```python\n",
    "from sklearn.decomposition import KernelPCA\n",
    "# 今回使うカーネル（動径基底関数）は、 kernel=\"rbf\" で指定できます。\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=15)\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "```\n",
    "\n",
    "　ここでは下図のような月形のデータの分離を行います。次のようにしてデータを取得します。\n",
    "\n",
    "```python\n",
    "from sklearn.datasets import make_moons\n",
    "# 月形データを取得\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "```\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap3_6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 空欄を埋めてKernelPCAクラスを用いて特徴変換を行い、月形のデータを線形分離可能なデータに変換しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "# KernelPCAをインポート\n",
    "# ---------------------------\n",
    "\n",
    "# ---------------------------\n",
    "\n",
    "# 月形データを取得\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "# KernelPCAクラスをインスタンス化\n",
    "kpca =\n",
    "# データXをKernelPCAを用いて変換\n",
    "X_kpca =\n",
    "\n",
    "# 可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.scatter(X[y == 0, 0], X[y == 0, 1], c=\"r\")\n",
    "ax1.scatter(X[y == 1, 0], X[y == 1, 1], c=\"b\")\n",
    "ax1.set_title(\"moon_data\")\n",
    "ax2.scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], c=\"r\")\n",
    "ax2.scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], c=\"b\")\n",
    "ax2.set_title(\"kernel_PCA\")\n",
    "plt.show()\n",
    "\n",
    "print(X_kpca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- KernelPCAクラスを、主成分分析の項で扱ったPCAと同じように適切に使うことで線形分離可能な特徴量に変換できます。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "# KernelPCAをインポート\n",
    "# ---------------------------\n",
    "from sklearn.decomposition import KernelPCA\n",
    "# ---------------------------\n",
    "\n",
    "# 月形データを取得\n",
    "X, y = make_moons(n_samples=100, random_state=123)\n",
    "\n",
    "# KernelPCAクラスをインスタンス化\n",
    "kpca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=15)\n",
    "# データXをKernelPCAを用いて変換\n",
    "X_kpca = kpca.fit_transform(X)\n",
    "\n",
    "# 可視化\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "ax1.scatter(X[y == 0, 0], X[y == 0, 1], c=\"r\")\n",
    "ax1.scatter(X[y == 1, 0], X[y == 1, 1], c=\"b\")\n",
    "ax1.set_title(\"moon_data\")\n",
    "ax2.scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], c=\"r\")\n",
    "ax2.scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], c=\"b\")\n",
    "ax2.set_title(\"kernel_PCA\")\n",
    "plt.show()\n",
    "\n",
    "print(X_kpca)  # 消さないでください。実行結果の確認に使います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 3.3 まとめ問題(提出不要)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　最後に、線形分離ができないデータをカーネルトリックを用いて、SVMにより分類することを考えます。まず、numpyのlogical_xor関数を使ってXORゲート形式のデータを用意します。100個のサンプルにラベル1を割り当て、その他の100個にラベル-1を割り当てます。このデータは線形分離できないため、カーネルトリックを用いてSVMで境界線を引きます。SVMでカーネルトリックを実現するには、scikit-learnのSVCクラスを利用し、パラメータkernel=\"linear\"を\"rbf\"に置き換えるだけで大丈夫です。\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "# カーネルSVMのインスタンスを生成\n",
    "kernel_svm = SVC(kernel=\"rbf\", C=10.0, random_state=0, gamma=.10)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下のコードを完成させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 乱数値を設定\n",
    "\n",
    "# 標準正規分布に従う乱数で200行2列の行列を生成\n",
    "\n",
    "# 2つの引数に対して排他的論理和を実行\n",
    "y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)\n",
    "# 排他的論理和の値が真の場合は1、偽の場合は-1を割り当てる\n",
    "y_xor = np.where(y_xor, 1, -1)\n",
    "# ラベル1を青のxでプロット\n",
    "\n",
    "# ラベル-1を赤の四角でプロット\n",
    "\n",
    "\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # カラーマップの準備\n",
    "    markers = (\"s\", \"x\", \"o\", \"^\", \"v\")\n",
    "    colors = (\"red\", \"blue\", \"lightgreen\", \"gray\", \"cyan\")\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # 決定領域のプロット\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    # グリッドポイントの生成\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    # 各特徴量を1次元配列に変換して予測を実行\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    # 予測結果を元のグリッドポイントのデータサイズに変換\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    # グリッドポイントの等高線のプロット\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    # 軸の範囲の設定\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # クラスごとにサンプルをプロット\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0],\n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.6,\n",
    "                    c=cmap(idx),\n",
    "                    edgecolor=\"black\",\n",
    "                    marker=markers[idx],\n",
    "                    label=cl)\n",
    "\n",
    "# RBFカーネルによるSVMのインスタンスを生成\n",
    "\n",
    "\n",
    "svm.fit(X_xor, y_xor)\n",
    "# 分類領域を可視化\n",
    "plot_decision_regions(X_xor, y_xor, classifier=svm)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- SVMのインスタンス化の際、引数kernelに\"rbf\"を指定すると、データに対しカーネルトリックを用いることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# 乱数値を設定\n",
    "np.random.seed(0)\n",
    "# 標準正規分布に従う乱数で200行2列の行列を生成\n",
    "X_xor = np.random.randn(200, 2)\n",
    "# 2つの引数に対して排他的論理和を実行\n",
    "y_xor = np.logical_xor(X_xor[:, 0] > 0, X_xor[:, 1] > 0)\n",
    "# 排他的論理和の値が真の場合は1、偽の場合は-1を割り当てる\n",
    "y_xor = np.where(y_xor, 1, -1)\n",
    "# ラベル1を青のxでプロット\n",
    "plt.scatter(X_xor[y_xor == 1, 0], X_xor[y_xor == 1, 1],\n",
    "            c=\"b\", marker=\"x\", label=\"1\")\n",
    "# ラベル-1を赤の四角でプロット\n",
    "plt.scatter(X_xor[y_xor == -1, 0], X_xor[y_xor == -1, 1],\n",
    "            c=\"r\", marker=\"s\", label=\"-1\")\n",
    "\n",
    "plt.xlim([-3, 3])\n",
    "plt.ylim([-3, 3])\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n",
    "\n",
    "    # カラーマップの準備\n",
    "    markers = (\"s\", \"x\", \"o\", \"^\", \"v\")\n",
    "    colors = (\"red\", \"blue\", \"lightgreen\", \"gray\", \"cyan\")\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # 決定領域のプロット\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    # グリッドポイントの生成\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    # 各特徴量を1次元配列に変換して予測を実行\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    # 予測結果を元のグリッドポイントのデータサイズに変換\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    # グリッドポイントの等高線のプロット\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    # 軸の範囲の設定\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # クラスごとにサンプルをプロット\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0],\n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.6,\n",
    "                    c=cmap(idx),\n",
    "                    edgecolor=\"black\",\n",
    "                    marker=markers[idx],\n",
    "                    label=cl)\n",
    "\n",
    "\n",
    "# RBFカーネルによるSVMのインスタンスを生成\n",
    "svm = SVC(kernel=\"rbf\", random_state=0, gamma=0.1, C=10.0)\n",
    "svm.fit(X_xor, y_xor)\n",
    "# 分類領域を可視化\n",
    "#plot_decision_regions(X_xor, y_xor, classifier=svm)\n",
    "#plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "commentary"
   },
   "source": [
    "　得られたグラフより、SVMでは分離不可能だったデータが、綺麗に分離されていることが確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
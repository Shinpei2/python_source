{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show()で可視化されない人はこのセルを実行してください。\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "r1cm_CQWgbz",
    "id": "chapter_name"
   },
   "source": [
    "#  非階層的クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[2.1 クラスタリングの技法](#2.1-クラスタリングの技法)**\n",
    "    - **[2.1.1 階層的クラスタリング](#2.1.1-階層的クラスタリング)**\n",
    "    - **[2.1.2 非階層的クラスタリング](#2.1.2-非階層的クラスタリング)**\n",
    "<br><br>\n",
    "- **[2.2 k-means法](#2.2-k-means法)**\n",
    "    - **[2.2.1 データの集まり](#2.2.1-データの集まり)**\n",
    "    - **[2.2.2 k-means法について](#2.2.2-k-means法について)**\n",
    "    - **[2.2.3 sklearnのKMeansライブラリ](#2.2.3-sklearnのKMeansライブラリ)**\n",
    "    - **[2.2.4 SSEについて](#2.2.4-SSEについて)**\n",
    "    - **[2.2.5 エルボー法](#2.2.5-エルボー法)**\n",
    "<br><br>\n",
    "- **[2.3 DBSCAN](#2.3-DBSCAN)**\n",
    "    - **[2.3.1 DBSCANのアルゴリズム](#2.3.1-DBSCANのアルゴリズム)**\n",
    "<br><br>\n",
    "- **[2.4 まとめ問題(提出不要)](#2.4-まとめ問題(提出不要))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "rJsmOAQWebf"
   },
   "source": [
    "## 2.1 クラスタリングの技法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "rJ5Z52IiUxf",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.1.1 階層的クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>階層的クラスタリング</b>とはデータの中から最も似ている組み合わせを探し出し、順番にクラスターにしていく方法であり、途中過程で<b>階層構造</b>になることが特徴としてあげられます。<br>\n",
    "<br>具体的な例として下の図を参照してください。5つのデータ点A,B,C,D,Eがあります。この5つのデータの内最も近いもの同士をまとめて、1つのクラスターを作るということを行います。<br>\n",
    "　　　A,B,C,D,E<br>\n",
    "　　　→ <b>(A,B),</b> C,D,E<br>\n",
    " 今の場合A,Bという2つがこの中の組み合わせでは最も近い点であると計算により判断されたので、<b>(A,B)</b>という1つのクラスターを作りました。\n",
    " \n",
    " 次に<b>新しく出来たクラスターも1つのデータ点とみなして、</b>これを繰り返します。<br>\n",
    "　　　→ <b>(A,B),</b> <b>(C,D),</b> E<br>\n",
    "　　　→ <b>(A,B,C,D),</b> E<br>\n",
    "　　　→ <b>(A,B,C,D,E)</b><br>\n",
    "最後に全データをまとめるクラスターまで行き着けば終了です。<br>\n",
    " データ点がどのクラスターにまとめられていったのかを表現したのが、下の右図のような<b>樹形図(デンドログラム)</b>です。\n",
    " \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap2_10.png\" width=500>\n",
    "\n",
    "<b><center>図2.1.1-1 樹形図(デンドログラム)</center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "以下の文章の「　」に当てはまる **言葉の組み合わせとして最も適切なもの** を以下の語群から選んでください。\n",
    "- 階層的クラスタリングにより、データから階層的な「　」が構築され、「　」が構成されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 「クラスター」「デンドログラム」\n",
    "- 「フォレスト」「クラスタリング」\n",
    "- 「フォレスト」「デンドログラム」\n",
    "- 「クラスター」「クラスタリング」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 階層的クラスタリングは、非階層的クラスタリングと異なり、階層構造を持つことが特徴です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 「クラスター」「デンドログラム」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "HJsZqhLi8gM",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.1.2 非階層的クラスタリング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>非階層的クラスタリング</b>も、階層的クラスタリングと同じくデータから似た性質のものを探し出し、クラスターを作りますが階層構造を持ちません。\n",
    "\n",
    "データが与えられた際、開発者があらかじめいくつのクラスターに分けるかを決定し、その数分だけデータからクラスターを作り出します。ただ、データごとに <b>最適なクラスター数は決まっておりません。</b>階層構造を持たないため階層的クラスタリングと比べると計算量が少なく、<b>データ量が多い場合に有効な手法</b>といえます。\n",
    "\n",
    "<b>非階層的クラスタリング</b>の代表的な手法である、<b>k-means法</b>について後ほどご紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "以下の文章の「　」に当てはまる **言葉として最も適切なもの** を以下の語群から選んでください。\n",
    "- 非階層的クラスタリングは、「　　」を持たず、あらかじめ分割する「　　」を決める必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 「対称構造」「階層数」\n",
    "- 「階層構造」「クラスター数」\n",
    "- 「階層構造」「階層数」\n",
    "- 「対称構造」「クラスター数」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 階層的クラスタリングの難点は、事前にクラスター数を決めてあげる必要があることです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "answer"
   },
   "source": [
    "- 「階層構造」「クラスター数」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "Hy3QdR7Zxbz"
   },
   "source": [
    "## 2.2 k-means法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "Sk3Wq3LjLxz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.1 データの集まり"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　本章で、クラスタリングについて解説していく前に、 **あらかじめデータ構造を保有している練習用のデータを用意します。** この項では、クラスター数を指定した分だけデータ内でクラスターが生成される `sklearn.datasets` 内の `make_blobs` 関数についてご紹介したいと思います。\n",
    "```python\n",
    "# sklearn.datasetsのmake_blobs関数をインポート\n",
    "from sklearn.datasets import make_blobs\n",
    "# Xには1つのプロットの(x,y)が、Yにはそのプロットの所属するクラスター番号が入る\n",
    "X,Y = make_blobs(n_samples=150,   # データ点の総数\n",
    "               n_features=2,          # 特徴量（次元数）の指定  default:2 \n",
    "               centers=3,             # クラスター数\n",
    "               cluster_std=0.5,       # クラスタ内の標準偏差 \n",
    "               shuffle=True,          # サンプルをシャッフル\n",
    "               random_state=0)        # 乱数生成器の状態を指定\n",
    "```\n",
    "上記のコードにより、**Xにデータ点** 、**Yにはそのデータ点が属するクラスターのラベル** が入ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下のコードの **クラスター数を変化させ** 、データの形状がどうなるか確認してみましょう。\n",
    "- `make_blobs()` の `centers= ` に様々な数字を入れて出力してみてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Xには1つのプロットの(x,y)が、Yにはそのプロットの所属するクラスター番号が入る\n",
    "X, Y = make_blobs(n_samples=150,       # サンプル点の総数\n",
    "                  n_features=2,          # 特徴量（次元数）の指定  default:2\n",
    "                  centers=__,            # ここを変えてください # クラスタの個数\n",
    "                  cluster_std=0.5,       # クラスタ内の標準偏差\n",
    "                  shuffle=True,          # サンプルをシャッフル\n",
    "                  random_state=0)        # 乱数生成器の状態を指定\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"black\", marker=\"*\", s=50)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "RESETをクリックすると元に戻ります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Xには1つのプロットの(x,y)が、Yにはそのプロットの所属するクラスター番号が入る\n",
    "X, Y = make_blobs(n_samples=150,       # サンプル点の総数\n",
    "                  n_features=2,          # 特徴量（次元数）の指定  default:2\n",
    "                  centers=10,            # クラスタの個数\n",
    "                  cluster_std=0.5,       # クラスタ内の標準偏差\n",
    "                  shuffle=True,          # サンプルをシャッフル\n",
    "                  random_state=0)        # 乱数生成器の状態を指定\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=\"black\", marker=\"*\", s=50)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "r16Z53LiLgz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.2 k-means法について"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "非階層的クラスタリングの代表的なものに、 <b style='color: #AA0000'>「k-means法」</b>があります。\n",
    "\n",
    "k-means法は、<b>データを分散の等しい$n$個のクラスター</b>に分けることができる手法です。各クラスターごとに<b>データの重心にあたる平均値$\\mu_i$</b>が割り当てられます。この重心のことを<b>「セントロイド」</b>と呼びます。分散の等しいクラスターに分けるには、<b>「SSE」</b>と呼ばれる指標を用います。SSEとは、各クラスターに含まれる<b>データ点とセントロイドとの差の2乗和</b>を求めたもの(分散にあたります)であり、<b style='color: #AA0000'> k-means法</b>はこのSSEを全クラスターで等しくかつ最小化するようにセントロイドを選びます。\n",
    "\n",
    "\n",
    "<b style='color: #AA0000'>k-means法</b>のアルゴリズムは、3つのステップがあります。\n",
    "<br>\n",
    "\n",
    "1.はじめに、データ群の中から$k$個(任意の数)のデータ点を抽出し、その<b>$k$個の点を初期のセントロイド</b>とします。セントロイドの初期化の後、2つのステップを反復します。<br>\n",
    "\n",
    "2.全てのデータ点を、最も近いセントロイドにそれぞれ割り振ります。<br>\n",
    "\n",
    "3.次に、各$k$個のセントロイドに割り振られたデータ群の<b>重心を計算</b>し、その重心を新たなセントロイドとして更新します。<br> \n",
    "\n",
    "\n",
    "ステップ3が終了する度に、前のセントロイドと新しくできたセントロイドの<b>距離を計算</b>します。その距離がある程度小さくなったら、上記の反復処理を終了します。言い換えれば、セントロイドが更新してもほとんど動かなくなるまで反復を行います。\n",
    " <br><br>\n",
    "　<b style='color: #AA0000'>k-means法</b>によりクラスタリングされたデータの例を以下に掲載します。\n",
    " \n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5030_unsupervised_learning/unsupervised_chap2_20.png\" width=500>\n",
    "\n",
    "<b><center>2.2.2-1 クラスタリング</center></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下のコードを実行することにより、 **k-means法の結果** を確認して見ましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# データセットの作成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "# k-means法を行います。\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "Y_km = km.fit_predict(X)  # Y_kmに各データ点が属するクラスタのラベルが入ります\n",
    "\n",
    "# グラフの描画\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "# 元データをプロット\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=\"black\")\n",
    "ax1.grid()\n",
    "# クラスタリング結果をプロット\n",
    "ax2.scatter(X[Y_km == 0, 0], X[Y_km == 0, 1], c=\"r\", s=40, label=\"cluster 1\")\n",
    "ax2.scatter(X[Y_km == 1, 0], X[Y_km == 1, 1], c=\"b\", s=40, label=\"cluster 2\")\n",
    "ax2.scatter(X[Y_km == 2, 0], X[Y_km == 2, 1], c=\"g\", s=40, label=\"cluster 3\")\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- コードを実行することにより、データの塊ごと色づけされていることがわかります。これは、データの塊ごとにクラスタリングされていることを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# データセットの作成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "# k-means法を行います。\n",
    "km = KMeans(n_clusters=3, random_state=0)\n",
    "Y_km = km.fit_predict(X)  # Y_kmに各データ点が属するクラスタのラベルが入ります\n",
    "\n",
    "# グラフの描画\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "# 元データをプロット\n",
    "ax1.scatter(X[:, 0], X[:, 1], c=\"black\")\n",
    "ax1.grid()\n",
    "# クラスタリング結果をプロット\n",
    "ax2.scatter(X[Y_km == 0, 0], X[Y_km == 0, 1], c=\"r\", s=40, label=\"cluster 1\")\n",
    "ax2.scatter(X[Y_km == 1, 0], X[Y_km == 1, 1], c=\"b\", s=40, label=\"cluster 2\")\n",
    "ax2.scatter(X[Y_km == 2, 0], X[Y_km == 2, 1], c=\"g\", s=40, label=\"cluster 3\")\n",
    "ax2.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "By0-52IiLeM",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.3 sklearnのKMeansライブラリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "前セクションのコードで使用した通り、`sklearn.cluster`の `KMeans` クラスは、データからクラスターを探し出し、各データにクラスタ番号を割り振ります。\n",
    "\n",
    "```python\n",
    "# sklearn.clusterのKMeansクラスをインポート\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3,            # クラスターの個数\n",
    "            init=\"random\",           # セントロイドの初期値をランダムに設定  default: \"k-means++\"\n",
    "            n_init=10,               # 異なるセントロイドの初期値を用いたk-meansの実行回数\n",
    "            max_iter=300,            # k-meansアルゴリズムを繰り返す最大回数\n",
    "            tol=1e-04,               # 収束と判定するための相対的な許容誤差\n",
    "            random_state=0)          # 乱数発生初期化\n",
    "    \n",
    "Y_km = km.fit_predict(X) # クラスターが存在するデータを渡し、各サンプルに対するクラスタ番号を求める\n",
    "```\n",
    "　上記のコードにより **データからクラスターを指定した分探し出し、Y_kmに各サンプルに自動的にクラスタ番号が格納されます。** `KMeans` クラスには他にも様々な関数があります。\n",
    "\n",
    "```python\n",
    "# クラスタリングの計算を実行\n",
    "km.fit(X[, y])\n",
    "# クラスタリングの計算を行い、Xを分析に用いた距離空間に変換して返す\n",
    "km.fit_transform(X[, y])\n",
    "# 計算に用いたパラメータを返す\n",
    "km.get_params([deep])\n",
    "# Xのサンプルが属しているクラスタ番号を返す\n",
    "km.predict(X)\n",
    "# パラメータを設定する\n",
    "km.set_params(**params)\n",
    "# Xを分析に用いた距離空間に変換して返す\n",
    "km.transform(X[, y])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下のコードを実行して、`KMeans` クラスが **どのようにデータのクラスタリングを行うか** をみてみましょう。\n",
    "- 適宜、`KMeans` クラスのクラスター数のパラメータを変更させ、クラスタリングが適切な場合と不適切な場合を確認しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# サンプルデータの生成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "# KMeansクラスからkmインスタンスを作成\n",
    "km = KMeans(n_clusters=3,            # クラスターの個数 # 変更してみてください\n",
    "            init=\"random\",           # セントロイドの初期値をランダムに設定  default: \"k-means++\"\n",
    "            n_init=10,               # 異なるセントロイドの初期値を用いたk-meansの実行回数\n",
    "            max_iter=300,            # k-meansアルゴリズムを繰り返す最大回数\n",
    "            tol=1e-04,               # 収束と判定するための相対的な許容誤差\n",
    "            random_state=0)          # 乱数発生初期化\n",
    "\n",
    "# fit_predictメソッドによりクラスタリングを行う\n",
    "Y_km = km.fit_predict(X)\n",
    "\n",
    "\n",
    "# クラスター番号(Y_km)に応じてデータをプロット\n",
    "for n in range(np.max(Y_km)+1):\n",
    "    plt.scatter(X[Y_km == n, 0], X[Y_km == n, 1], s=50, c=cm.hsv(\n",
    "        float(n) / 10), marker=\"*\", label=\"cluster\"+str(n+1))\n",
    "\n",
    "# セントロイドをプロット、km.cluster_centers_には各クラスターのセントロイドの座標が入っています\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[\n",
    "            :, 1], s=250, marker=\"*\", c=\"black\", label=\"centroids\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.7), loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- make_blobs関数にて設定したクラスター数と、KMeansクラスのパラメータとして設定したクラスター数を一致させると、クラスタリングが適切に行われます。異なっていると、正しいクラスタリングができないケースが多いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# サンプルデータの生成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "# KMeansクラスからkmインスタンスを作成\n",
    "km = KMeans(n_clusters=3,            # クラスターの個数 # 変更してみてください\n",
    "            init=\"random\",           # セントロイドの初期値をランダムに設定  default: \"k-means++\"\n",
    "            n_init=10,               # 異なるセントロイドの初期値を用いたk-meansの実行回数\n",
    "            max_iter=300,            # k-meansアルゴリズムを繰り返す最大回数\n",
    "            tol=1e-04,               # 収束と判定するための相対的な許容誤差\n",
    "            random_state=0)          # 乱数発生初期化\n",
    "\n",
    "# fit_predictメソッドによりクラスタリングを行う\n",
    "Y_km = km.fit_predict(X)\n",
    "\n",
    "\n",
    "# クラスター番号(Y_km)に応じてデータをプロット\n",
    "for n in range(np.max(Y_km)+1):\n",
    "    plt.scatter(X[Y_km == n, 0], X[Y_km == n, 1], s=50, c=cm.hsv(\n",
    "        float(n) / 10), marker=\"*\", label=\"cluster\"+str(n+1))\n",
    "\n",
    "# セントロイドをプロット、km.cluster_centers_には各クラスターのセントロイドの座標が入っています\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[\n",
    "            :, 1], s=250, marker=\"*\", c=\"black\", label=\"centroids\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.7), loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "Bkkz5nLsIlf",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.4 SSEについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "　クラスタリングの性能評価関数の一つに<b>SSE</b>(クラスタ内誤差平方和)がありますSSEを用いることにより様々なk-meansクラスタリングの<b>性能を評価</b>することができます。 SSEの数式は省略しますが、SSEの値はクラスタ内の値がどれだけ離れているのかを示します。\n",
    " \n",
    "sklearnでは `KMeans` クラスの`inertia_` 属性を通じてSSEの値を取得できます 。各データが自身の属する<b>クラスター重心からどれほどずれているか</b>(分散)の総和が<b>SSE</b>であるため、SSEの値が小さいほどクラスタリングがうまくいっているモデルと言えます。\n",
    " \n",
    "```python\n",
    "# クラスタ内誤差平方和にアクセスする\n",
    "print (\"Distortion: %.2f\"% km.inertia_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下のコードを実行することにより、**SSEの値を確認** してみましょう。また `KMeans` のクラスター数を変更すると **SSEの値がどのように変化するか** 確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# サンプルデータの生成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "# KMeansクラスからkmインスタンスを作成\n",
    "km = KMeans(n_clusters=3,            # クラスターの個数 # 変更してみてください\n",
    "            init=\"random\",           # セントロイドの初期値をランダムに設定  default: \"k-means++\"\n",
    "            n_init=1,               # 異なるセントロイドの初期値を用いたk-meansの実行回数\n",
    "            max_iter=300,            # k-meansアルゴリズムを繰り返す最大回数\n",
    "            tol=1e-04,               # 収束と判定するための相対的な許容誤差\n",
    "            random_state=0)          # 乱数発生初期化\n",
    "\n",
    "# fit_predictメソッドによりクラスタリングを行う\n",
    "Y_km = km.fit_predict(X)\n",
    "\n",
    "# SSE値を出力\n",
    "print(\"Distortion: %.2f\" % km.inertia_)\n",
    "\n",
    "# プロット\n",
    "for n in range(np.max(Y_km)+1):\n",
    "    plt.scatter(X[Y_km == n, 0], X[Y_km == n, 1], s=50, c=cm.hsv(\n",
    "        float(n) / 10), marker=\"*\", label=\"cluster\"+str(n+1))\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[\n",
    "            :, 1], s=250, marker=\"*\", c=\"black\", label=\"centroids\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.7), loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 基本的にSSEの値が小さい程、クラスタリングがうまくいっていると言えますが、SSEの定義の関係から基本的にクラスター数が増えるほど値が小さくなっていきます。真に良いクラスター数の判断は、次のエルボー法によって行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# サンプルデータの生成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "# KMeansクラスからkmインスタンスを作成\n",
    "km = KMeans(n_clusters=3,            # クラスターの個数 # 変更してみてください\n",
    "            init=\"random\",           # セントロイドの初期値をランダムに設定  default: \"k-means++\"\n",
    "            n_init=1,               # 異なるセントロイドの初期値を用いたk-meansの実行回数\n",
    "            max_iter=300,            # k-meansアルゴリズムを繰り返す最大回数\n",
    "            tol=1e-04,               # 収束と判定するための相対的な許容誤差\n",
    "            random_state=0)          # 乱数発生初期化\n",
    "\n",
    "# fit_predictメソッドによりクラスタリングを行う\n",
    "Y_km = km.fit_predict(X)\n",
    "\n",
    "# SSE値を出力\n",
    "print(\"Distortion: %.2f\" % km.inertia_)\n",
    "\n",
    "# プロット\n",
    "for n in range(np.max(Y_km)+1):\n",
    "    plt.scatter(X[Y_km == n, 0], X[Y_km == n, 1], s=50, c=cm.hsv(\n",
    "        float(n) / 10), marker=\"*\", label=\"cluster\"+str(n+1))\n",
    "plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[\n",
    "            :, 1], s=250, marker=\"*\", c=\"black\", label=\"centroids\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.7), loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "r1lz5hUo8ef",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.5 エルボー法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "k-meansクラスタリングで指定するクラスター数はどう決めれば良いかといった問題があります。\n",
    "\n",
    "このクラスター数を決定する時に参考になる手法があります。これは、<b style='color: #AA0000'>エルボー法</b>と呼ばれ、<b>クラスタ数を大きく</b>していった時にSSEがどのように変化するかプロットし、その結果からk-meansのクラスタ数を決定する手法です。問題にあるコードを実行すればわかりますが、SSEの値がガクンと曲がる点があります。この時のクラスター数が<b>最適なものとみなす</b>ことができます。<b>プロットの形状が肘が曲がっているように見える</b>ことから、エルボー法と呼ばれています。ただ、現実的には問題の結果図のような綺麗にある点でグラフが落ち込むようなエルボー図が得られることはなかなかありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次のコードを実行することにより、**エルボー図の概略** を掴みましょう。また `KMeans`  クラスのクラスター数の幅を広げ、**エルボー法に変化があるか** 確認してみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# サンプルデータの生成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "distortions = []\n",
    "for i in range(1, 11):                # クラスター数1~10を一気に計算\n",
    "    km = KMeans(n_clusters=i,\n",
    "                init=\"k-means++\",     # k-means++法によりクラスタ中心を選択\n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=0)\n",
    "    km.fit(X)                         # クラスタリングのを実行\n",
    "    distortions.append(km.inertia_)   # km.fitするとkm.inertia_が得られる\n",
    "\n",
    "# グラフのプロット\n",
    "plt.plot(range(1, 11), distortions, marker=\"o\")\n",
    "plt.xticks(np.arange(1, 11, 1))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Distortion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- エルボー図においては、SSEの値が一度サチる(飽和する)と改善することはほとんどありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# サンプルデータの生成\n",
    "X, Y = make_blobs(n_samples=150, n_features=2, centers=3,\n",
    "                  cluster_std=0.5, shuffle=True, random_state=0)\n",
    "\n",
    "distortions = []\n",
    "for i in range(1, 11):                # クラスター数1~10を一気に計算\n",
    "    km = KMeans(n_clusters=i,\n",
    "                init=\"k-means++\",     # k-means++法によりクラスタ中心を選択\n",
    "                n_init=10,\n",
    "                max_iter=300,\n",
    "                random_state=0)\n",
    "    km.fit(X)                         # クラスタリングのを実行\n",
    "    distortions.append(km.inertia_)   # km.fitするとkm.inertia_が得られる\n",
    "\n",
    "# グラフのプロット\n",
    "plt.plot(range(1, 11), distortions, marker=\"o\")\n",
    "plt.xticks(np.arange(1, 11, 1))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Distortion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "Sk6mOA7-e-G"
   },
   "source": [
    "## 2.3 DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5030,
    "exerciseId": "r1ZM53IjUgz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.3.1 DBSCANのアルゴリズム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "今まで見てきたk-means法は、<b>クラスター中心に出来るだけデータが集まる</b>ようにクラスタリングしていました。そのため、必然的にクラスターは<b>円形</b>(球状)に近い形を取ります。クラスターの大きさ・形に<b>偏りがない</b>ときは効果を発揮しますが、クラスターの大きさ・形に<b>偏りがある</b>データの場合は良いクラスタリングができない傾向にあります。<br>\n",
    " \n",
    "k-means法に対して、別の非階層クラスタリングのアルゴリズムに <b style='color: #AA0000'>「DBSCAN」</b>があります。<b style='color: #AA0000'>「DBSCAN」</b>のアルゴリズムは、 クラスターの高密度(データが凝集している)の場所を低密度の場所から分離して捉えます。クラスターサイズ・形に<b>偏りがある</b>際に真価を発揮します。\n",
    "  \n",
    "　<b style='color: #AA0000'>「DBSCAN」</b>では、2つのパラメータを定義します。 `min_samples`と`eps`です。 <b style='color: #AA0000'>「DBSCAN」</b>のアルゴリズムでは、次の3種類にデータ点を分類します\n",
    "\n",
    " 1.あるデータの半径 `eps` 内に `min_sample` 数だけのデータがある場合、そのデータ点は **コア点** とみなします。<br>\n",
    "\n",
    " 2.また、コア点ではないが、コア点から半径 `eps` 内に入っているデータは、ボーダー点とみなします。 <br>\n",
    "\n",
    " 3.どちらにも満たさないデータ点は、ノイズ点とみなします。\n",
    "\n",
    "コア点の集まりからクラスターを形成します。<b>ボーダー点</b>は、最も近いコア点の属するクラスターに割り振られます。このように <b style='color: #AA0000'>「DBSCAN」</b>のアルゴリズムでは、全データを3つのデータに分類することにより、偏ったデータや、平均的ではないクラスターも分類できるようになり、ノイズを正しく除去することもできます。\n",
    " \n",
    "　<b style='color: #AA0000'>「DBSCAN」</b> は、`sklearn.cluster`の`DBSCAN`クラスを利用することができます。主なパラメータとして、`eps`、`min_sample`、 `metric`により距離計算法を指定します。\n",
    " \n",
    "```python\n",
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=0.2,\n",
    "            min_samples=5,\n",
    "            metric=\"euclidean\")\n",
    "Y_db = db.fit_predict(X)\n",
    "```\n",
    "\n",
    "問題で、k-means法とDBSCANの分類結果を比べて見ましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次のコードを完成することにより、**k-means法とDBSCANの分類結果の相違を確認**してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# 月型のデータを生成\n",
    "X, Y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# グラフと2つの軸を定義 左はk-means法用、右はDBSCAN用\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# k-means法\n",
    "km = KMeans(n_clusters=2, random_state=0)\n",
    "Y_km = km.fit_predict(X)\n",
    "\n",
    "ax1.scatter(X[Y_km == 0, 0], X[Y_km == 0, 1], c=\"lightblue\",\n",
    "            marker=\"o\", s=40, label=\"cluster 1\")\n",
    "ax1.scatter(X[Y_km == 1, 0], X[Y_km == 1, 1], c=\"red\",\n",
    "            marker=\"s\", s=40, label=\"cluster 2\")\n",
    "ax1.set_title(\"K-means clustering\")\n",
    "\n",
    "# DBSCANでクラスタリング # コードを完成してください\n",
    "db =\n",
    "Y_db =\n",
    "\n",
    "ax2.scatter(X[Y_db == 0, 0], X[Y_db == 0, 1], c=\"lightblue\",\n",
    "            marker=\"o\", s=40, label=\"cluster 1\")\n",
    "ax2.scatter(X[Y_db == 1, 0], X[Y_db == 1, 1], c=\"red\",\n",
    "            marker=\"s\", s=40, label=\"cluster 2\")\n",
    "ax2.set_title(\"DBSCAN clustering\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- k-means法は、今回のような複雑な形状をもったデータに弱い反面、DBSCANは、任意の形状をクラスタリングすることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# 月型のデータを生成\n",
    "X, Y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "\n",
    "# グラフと2つの軸を定義 左はk-means法用、右はDBSCAN用\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# k-means法\n",
    "km = KMeans(n_clusters=2, random_state=0)\n",
    "Y_km = km.fit_predict(X)\n",
    "\n",
    "ax1.scatter(X[Y_km == 0, 0], X[Y_km == 0, 1], c=\"lightblue\",\n",
    "            marker=\"o\", s=40, label=\"cluster 1\")\n",
    "ax1.scatter(X[Y_km == 1, 0], X[Y_km == 1, 1], c=\"red\",\n",
    "            marker=\"s\", s=40, label=\"cluster 2\")\n",
    "ax1.set_title(\"K-means clustering\")\n",
    "\n",
    "# DBSCANでクラスタリング\n",
    "db = DBSCAN(eps=0.2, min_samples=5, metric=\"euclidean\")\n",
    "Y_db = db.fit_predict(X)\n",
    "\n",
    "ax2.scatter(X[Y_db == 0, 0], X[Y_db == 0, 1], c=\"lightblue\",\n",
    "            marker=\"o\", s=40, label=\"cluster 1\")\n",
    "ax2.scatter(X[Y_db == 1, 0], X[Y_db == 1, 1], c=\"red\",\n",
    "            marker=\"s\", s=40, label=\"cluster 2\")\n",
    "ax2.set_title(\"DBSCAN clustering\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 2.4 まとめ問題(提出不要)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    " 円状のデータのクラスタリングにおいて、k-means法とDBSCANによる結果の違いを見て見ましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- sklearn.datasetsモジュールの「make_circles」関数を利用し、円状のデータを生成させた後、それらをk-means法とDBSCANを用いてクラスタリング、可視化しましょう。なお、結果図は、左にk-means法によるクラスタリング結果、右にDBSCANによるクラスタリング結果が表示されるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# 円状のデータを生成\n",
    "\n",
    "# figureオブジェクトの生成、並びに2軸の定義\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# k-meansのインスタンス化\n",
    "\n",
    "\n",
    "# DBSCANのインスタンス化\n",
    "\n",
    "\n",
    "# 左にk-means法によるクラスタリングの結果を表示\n",
    "\n",
    "\n",
    "# 右にDBSCANによるクラスタリングの結果を表示\n",
    "\n",
    "# 可視化\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 円状のデータは、X, y = make_circles(n_samples=150, random_state=4, noise=0.05, factor=0.5)と書くことで、計算することができます。\n",
    "- DBSCANは、パラメータeps, min_samplesの値によってクラスタリングの結果が著しく変わるため、調整が必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# 円状のデータを生成\n",
    "X, y = make_circles(n_samples=150, random_state=4, noise=0.05, factor=0.5)\n",
    "# figureオブジェクトの生成、並びに2軸の定義\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n",
    "\n",
    "# k-meansのインスタンス化\n",
    "km = KMeans(n_clusters=2, random_state=0)\n",
    "Y_km = km.fit_predict(X)\n",
    "\n",
    "# DBSCANのインスタンス化\n",
    "db = DBSCAN(eps=0.3, min_samples=7, metric=\"euclidean\")\n",
    "Y_db = db.fit_predict(X)\n",
    "\n",
    "# 左にk-means法によるクラスタリングの結果を表示\n",
    "ax1.scatter(X[Y_km == 0, 0], X[Y_km == 0, 1])\n",
    "ax1.scatter(X[Y_km == 1, 0], X[Y_km == 1, 1])\n",
    "ax1.set_title(\"k-means\")\n",
    "\n",
    "# 右にDBSCANによるクラスタリングの結果を表示\n",
    "ax2.scatter(X[Y_db == 0, 0], X[Y_db == 0, 1])\n",
    "ax2.scatter(X[Y_db == 1, 0], X[Y_db == 1, 1])\n",
    "ax2.set_title(\"DBSCAN\")\n",
    "\n",
    "# 可視化\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解説"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "commentary"
   },
   "source": [
    "　k-means法は、全てのクラスターを同じサイズに統一しようとする性質があり結果図のように、円状のデータが半分に分かれていることがわかります。つまりこのようなデータには、不向きです。反対に、DBSCANは不均一なデータにも対処できるため、結果図のように円状のデータも綺麗に分割できることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

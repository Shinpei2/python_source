{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show()で可視化されない人はこのセルを実行してください。\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "ByNLu0mZx-z",
    "id": "chapter_name"
   },
   "source": [
    "#  深層学習のチューニング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[2.1 ハイパーパラメータ](#2.1-ハイパーパラメータ)**\n",
    "    - **[2.1.1 ハイパーパラメータ](#2.1.1-ハイパーパラメータ)**\n",
    "<br><br>\n",
    "- **[2.2 ネットワーク構造](#2.2-ネットワーク構造)**\n",
    "    - **[2.2.1 ネットワーク構造](#2.2.1-ネットワーク構造)**\n",
    "<br><br>\n",
    "- **[2.3 ドロップアウト](#2.3-ドロップアウト)**\n",
    "    - **[2.3.1 ドロップアウト](#2.3.1-ドロップアウト)**\n",
    "<br><br>\n",
    "- **[2.4 活性化関数](#2.4-活性化関数)**\n",
    "    - **[2.4.1 活性化関数](#2.4.1-活性化関数)**\n",
    "    - **[2.4.2 シグモイド関数](#2.4.2-シグモイド関数)**\n",
    "    - **[2.4.3 ReLU](#2.4.3-ReLU)**\n",
    "<br><br>\n",
    "- **[2.5 損失関数](#2.5-損失関数)**\n",
    "    - **[2.5.1 損失関数](#2.5.1-損失関数)**\n",
    "    - **[2.5.2 二乗誤差](#2.5.2-二乗誤差)**\n",
    "    - **[2.5.3 クロスエントロピー誤差](#2.5.3-クロスエントロピー誤差)**\n",
    "<br><br>\n",
    "- **[2.6 最適化関数](#2.6-最適化関数)**\n",
    "    - **[2.6.1 最適化関数](#2.6.1-最適化関数)**\n",
    "<br><br>\n",
    "- **[2.7 学習率](#2.7-学習率)**\n",
    "    - **[2.7.1 学習率](#2.7.1-学習率)**\n",
    "<br><br>\n",
    "- **[2.8 ミニバッチ学習](#2.8-ミニバッチ学習)**\n",
    "    - **[2.8.1 ミニバッチ学習](#2.8.1-ミニバッチ学習)**\n",
    "<br><br>\n",
    "- **[2.9 反復学習](#2.9-反復学習)**\n",
    "    - **[2.9.1 反復学習](#2.9.1-反復学習)**\n",
    "<br><br>\n",
    "- **[2.10 まとめ問題(提出不要)](#2.10-まとめ問題(提出不要))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "SyH8uRXZebf"
   },
   "source": [
    "## 2.1 ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "Syd8938sUlf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.1.1 ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "ニューラルネットワークモデルには、ネットワークを構成する際に調整が必要となる<b style='color: #AA0000'>ハイパーパラメータ</b>と呼ばれるものがいくつか存在します。\n",
    "\n",
    "ハイパーパラメータは数多く存在し、適切に設定しないと正しく学習が行われません。そこで、新規モデル作成時には最適なハイパーパラメータを設計する必要があります。<br>\n",
    "\n",
    "実際のコードでハイパーパラメータを見てみましょう。\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "# ハイパーパラメータ：活性化関数\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# ハイパーパラメータ：隠れ層の数、隠れ層のユニット数\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# ハイパーパラメータ：ドロップアウトする割合（rate）\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "# ハイパーパラメータ：学習率（lr）\n",
    "sgd = optimizers.SGD(lr=0.01)\n",
    "\n",
    "# ハイパーパラメータ：最適化関数（optimizer）\n",
    "# ハイパーパラメータ：誤差関数（loss）\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# ハイパーパラメータ：バッチサイズ（batch_size）\n",
    "# ハイパーパラメータ：エポック数（epochs）\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))\n",
    "```\n",
    "\n",
    "上記は、<a href='https://aidemy.net/courses/5090/exercises/H1PL928jIxf' target='_blank'>Chapter1</a>のMNIST分類のコードに少しだけ変更を加え、いくつかのパラメータを明示したものです。なお、`metrics`は評価関数です。評価関数については<a href='https://aidemy.net/courses/2010' target='_blank'>機械学習概論</a>を参照してください。\n",
    "\n",
    "このChapterではそれぞれのハイパーパラメータの意味を理解し、ネットワークの構築、調整の方法を学んでいきましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- ハイパーパラメータについて説明した文として正しいものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- ハイパーパラメータは学習時にモデルが自動的に調整する。\n",
    "- ハイパーパラメータは自分で調整する必要がある。\n",
    "- ハイパーパラメータは適切に設定するのが良いが、適切でなくとも多くの場合問題なく学習は進行する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- パラメーターのうち人が調整するパラメーターをハイパーパラメーターと言います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- ハイパーパラメータは自分で調整する必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "rkIIOA7bgWz"
   },
   "source": [
    "## 2.2 ネットワーク構造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BkKUqhIoUgz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.2.1 ネットワーク構造の設定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "入力層と出力層の間にある **隠れ層** の数やユニット数は自由に指定することが可能で、数を多くすることによって多彩な関数を表現できるようになります。<br>\n",
    "\n",
    "しかし、隠れ層の数が多い場合は、重みの調整の難易度が上がって **学習の進行が遅く** なったり、ユニット数が多い場合は、重要度の低い特徴量を抽出して **過学習** (汎化性能が低い状態)を起こしやすくなったりします。\n",
    "したがって、ただやみくもに数を増やすのではなく、学習に適切な数を設定することが必要です。\n",
    "\n",
    "同じような実装例を参考にするなど前例に基づいてネットワーク構造を検討します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "隠れ層の構造がモデルの学習に与える影響を確認し、次の3つの中から一番精度の高いモデルを予想してみましょう。\n",
    "\n",
    " - A: ユニット数256の全結合隠れ層1つ、ユニット数128の全結合隠れ層1つ\n",
    " - B: ユニット数256の全結合隠れ層1つ、ユニット数128の全結合隠れ層3つ\n",
    " - C: ユニット数256の全結合隠れ層1つ、ユニット数1568の全結合隠れ層1つ\n",
    "\n",
    "コード内の指示にしたがって、コードを変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcA():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcB():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcC():\n",
    "    model.add(Dense(1568))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "#A、B、Cのモデルの中から1つを選び、残りの2つはコメントアウトしてください。\n",
    "#---------------------------\n",
    "funcA()\n",
    "funcB()\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全パターン試してみてください。\n",
    "- evaluate acc: が精度を表しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcA():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcB():\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "def funcC():\n",
    "    model.add(Dense(1568))\n",
    "    model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# A、B、Cのモデルの中から1つを選び、残りの2つはコメントアウトしてください。\n",
    "#---------------------------\n",
    "funcA()\n",
    "#funcB()\n",
    "#funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "BkPIOAXZebG"
   },
   "source": [
    "## 2.3 ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "HJqI92UsIgG",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.3.1 ドロップアウト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>ドロップアウト</b>は、 **学習データに対する過学習を防ぎ、モデルの精度をあげるための手法の一つ** です。  \n",
    "\n",
    "ドロップアウトではランダムにニューロンを削除（0で上書き）しながら、学習を繰り返します。それによって、ニューラルネットワークは特定のニューロンに依存することなく、より **汎用的な特徴を学習する** ようになります。 \n",
    "\n",
    "ドロップアウトの記述は下記のようになります。\n",
    "```python\n",
    "model.add(Dropout(rate=0.5))\n",
    "```\n",
    "rateは削除するユニットの割合です。\n",
    "\n",
    "**ドロップアウトの位置やrateについては、どちらもハイパーパラメータとなります。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "ドロップアウトを実装し、訓練データとテストデータ、それぞれの正解率を近づけましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "# ---------------------------\n",
    "# ここに先ほどのドロップアウトのコードを記述します\n",
    "\n",
    "# ---------------------------\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `Dropout()`を使用してドロップアウトを実装します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "#- --------------------------\n",
    "# ここを書いて下さい\n",
    "model.add(Dropout(rate=0.5))\n",
    "# ここまで書いて下さい\n",
    "# ---------------------------\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=5, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "BJuLO0QbgZG"
   },
   "source": [
    "## 2.4 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "SJo89hLjLgf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.1 活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>活性化関数</b>とは、全結合層などの後に適用する関数で、ニューロンの**発火**に相当するものです。  \n",
    "\n",
    "全結合層では、入力を線形変換して出力しますが、 **活性化関数を用いることで非線形性をもたせることができます** 。\n",
    "\n",
    "活性化関数を使用しないと、下図のように**一本の直線で分離できず（線形分離不可能）、データが分類できなくなる** のです。\n",
    "\n",
    "<img src='https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_10.png'>\n",
    "\n",
    "**<center>図2.4.1-1 線形分離不可能な散布図</center>**\n",
    "\n",
    "活性化関数によって非線形性をもたせることで、 **線形分離不可能なモデルでも、適切に学習が進めば必ず分類できるようになります** 。\n",
    "\n",
    "なお、活性化関数にもいくつかの種類があるので、適切なものを選ぶことが大切です。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 活性化関数を使う理由として正しい選択肢を選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- モデルに線形性をもたせ、線形分離可能なデータに対応させるため。\n",
    "- モデルに線形性をもたせ、線形分離不可能なデータに対応させるため。\n",
    "- モデルに非線形性をもたせ、線形分離可能なデータに対応させるため。\n",
    "- モデルに非線形性をもたせ、線形分離不可能なデータに対応させるため。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- モデルが線型性の場合は、線形分離不可能なデータを分類できません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- モデルに非線形性をもたせ、線形分離不可能なデータに対応させるため。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "By2Ucn8jIlf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.2 シグモイド関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>シグモイド関数</b>は活性化関数のひとつで、入力値を0から1の間の数値にして出力します。下記のような式になります。\n",
    "\n",
    "$$\n",
    "sigmoid(x) = \\frac{1}{1+e^{-x}}\n",
    "$$\n",
    "\n",
    "<img src='https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_20.png'>\n",
    "\n",
    "**<center>図2.4.2-1 シグモイド関数</center>**\n",
    "\n",
    "青いグラフが **シグモイド関数** で、オレンジ色のグラフがシグモイド関数を微分した **導関数** です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 説明文のグラフからわかる、シグモイド関数の説明として正しいものを1つ選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 出力は必ず区間 `(0,1)` に収まるので、極端な出力値が少ない。\n",
    "- どのような区間にも収まらず、極端な出力値が生成される可能性がある。 \n",
    "- 出力が広い値をとるので、学習速度が早くなる。\n",
    "- 出力の範囲が限られていないので、学習速度が遅くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 縦軸の値に注目しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "出力は必ず区間 `(0,1)` に収まるので、極端な出力値が少ない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BkpIqhLjUgf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.4.3 ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "よく使われる活性化関数として <b style='color: #AA0000'>ReLU関数（ランプ関数）</b>があります。この関数は入力する値が0以下であれば0を、1以上であれば値をそのまま入力値を出力します。 \n",
    "ReLUは、**Rectified Linear Unit**の略で、下記のような式となります。\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(x) = \\begin{cases} 0 (x<0) \\\\ x (x\\geq 0)\\end{cases}\n",
    "$$\n",
    "\n",
    "<img src='https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_30.png'>\n",
    "\n",
    "**<center>図2.4.3-1 ReLU関数</center>**\n",
    "\n",
    "青いグラフが **ReLU関数** で、オレンジ色のグラフが **導関数** です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 説明文のグラフからわかる、ReLUの説明として正しいものを1つ選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 出力は必ず区間(0,1)に収まるので、極端な出力値が少ない。\n",
    "- 出力はどのような区間にも収まらず、極端な出力値が生成されうる。\n",
    "- 出力が広い値をとるので、学習速度が遅くなる。\n",
    "- 出力の範囲が限られているので、学習速度が遅くなる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 一般的に、出力が大きい値だと学習速度は早くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 出力はどのような区間にも収まらず、極端な出力値が生成されうる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "rktIOCX-gZz"
   },
   "source": [
    "## 2.5 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "S1A85hIsIxz",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.5.1 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "学習時に出力データと教師データとの差を評価する関数を <b style='color: #AA0000'>損失関数（誤差関数）</b>といいます。\n",
    "\n",
    "正解率を指標に評価することも可能ですが、それだけでは個々の出力データの正解・不正解といった細かい結果までは分かりません。<br>\n",
    "つまり、一つひとつの出力データと教師データの差をみるために **損失関数** が使われるのです。\n",
    "\n",
    "損失関数にはたくさんの種類がありますが、機械学習によく使われるものとしては **二乗誤差** や **クロスエントロピー誤差** などが挙げられます。<br>\n",
    "この2つについては、後ほど詳しく説明します。\n",
    "\n",
    "また、損失関数の微分の計算を効率的にするために誤差逆伝播法という手法が使われます。この手法では、出力データと教師データの差を **最小化** するように各層の **重みを更新** します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    " - 損失関数の性質を示した文として適切なものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 一般に、損失関数を最大化するように各層の重みを更新する。\n",
    "- 損失関数は重みを更新する際に重要な役割を持つので、適切なものを選ぶ必要がある。\n",
    "- 損失関数には正解率を求める式をそのまま使うのが良い。\n",
    "- 損失関数は1種類しかないので、これはハイパーパラメータではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 損失関数には様々な種類が存在し、値を最小化するように重みの更新をします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 損失関数は重みを更新する際に重要な役割を持つので、適切なものを選ぶ必要がある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BJJvch8iIlG",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### 2.5.2 二乗誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>平均平方二乗誤差</b>は最小二乗法と並んで、統計学などの分野でよく使用される誤差関数です。\n",
    "下記の式で記述します。\n",
    "\n",
    "$$\n",
    "{E} = \\sum_{i=1}^N ({t_i - y_i})^{2}\n",
    "$$\n",
    "\n",
    "<img src='https://aidemyexstorage.blob.core.windows.net/aidemycontents/1543300574968104.png' width=400>\n",
    "\n",
    "<b><center>誤差 = (<b style='color: #AA0000'>2</b>- <b style='color: 0000AA'>3</b>$)^2$ + (<b style='color: #AA0000'>4</b>- <b style='color: #0000AA'>5</b>$)^2$ + (<b style='color: #AA0000'>6</b>- <b style='color: #0000AA'>8</b>$)^2$ = 6</center></b>\n",
    "\n",
    "<b><center>図2.5.2-1　二乗誤差</center></b>\n",
    "\n",
    "$y_i$は **予測ラベル** 、$t_i$は **正解ラベル** です。平均平方二乗誤差は連続値の評価を得意とするため、 **主に回帰モデルの誤差関数** として使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 平均平方二乗誤差の説明として正しいものを1つ選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 回帰に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しやすい\n",
    "- 回帰に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しにくい\n",
    "- 分類に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しやすい\n",
    "- 分類に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しにくい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 下に凸の放物線をイメージしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 回帰に向いており、最小値の付近ではゆっくりと更新が行われるため、学習が収束しやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "rygv9n8i8xf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.5.3 クロスエントロピー誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>クロスエントロピー誤差</b>は、 **分類の評価に特化** しているため、主に **分類モデルの誤差関数** として使われます。<br>\n",
    "式は下記の通りになります。<br>\n",
    "\n",
    "$$\n",
    "E=\\sum_{i=1}^N (-t_i\\log y_i-(1-t_i)\\log (1-y_i))\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "また、一般的には以下の式が使用されることが多いです。<br>\n",
    "\n",
    "\n",
    "$$\n",
    "E=-\\sum_{i=1}^N t_i\\log y_i\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "そうすることによって、ラベルが`1`以外の項はすべて0になるため、<b>実質的に正解ラベルの誤差のみを計算していることになります。</b><br>\n",
    "\n",
    "<figure>\n",
    "    <img src='https://aidemyexstorage.blob.core.windows.net/aidemycontents/1561008430395325.png' width=500>\n",
    "    <figcaption><center><b>2種分類の出力層</b></center></figcaption>\n",
    "<figure>\n",
    "    \n",
    "<br>\n",
    "\n",
    "たとえば、上記のような出力があったとします。このクロスエントロピー誤差を計算すると以下のようになります。<br>\n",
    "\n",
    "\n",
    "$$\n",
    "E = -1\\log0.8 - 0\\log0.2 = 0.097 + 0 = 0.097\n",
    "$$\n",
    "\n",
    "\n",
    "つまり、$y_i$が1に近づくほど$logy_i$が0に近づくので誤差が小さくなり、yが0に近づくほど$logy_i$が-∞に近づくので誤差が大きくなるという原理であることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- クロスエントロピー誤差について正しい選択肢を選んでください。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 正解ラベルと予測ラベルの値が近いほど小さい値となる。\n",
    "- 多クラス分類に特化した誤差関数である。\n",
    "- 回帰問題に頻繁に用いられる誤差関数である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- クロスエントロピーは二値分類の評価に特化しており、主に分類モデルの誤差関数として使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 正解ラベルと予測ラベルの値が近いほど小さい値となる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "rJ5LOAXWgbf"
   },
   "source": [
    "## 2.6 最適化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "rJWv53IsUlG",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.6.1 最適化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "誤差関数で微分した値を元に、方向と程度を決めて重みを更新します。<br>\n",
    "その際、 **学習率、エポック数、過去の重みの更新量など** をどのように重みの更新に反映するかを定めるために使用するのが <b style='color: #AA0000'>最適化関数</b>です。\n",
    "<a href='https://aidemy.net/courses/5090/exercises/HkMP93IoLlM' target='_blank'>学習率</a>、<a href='https://aidemy.net/courses/5090/exercises/Sy4v9n8oUxz' target='_blank'>エポック数</a>についての詳細は「ディープランニングの基礎」で説明しています。<br>\n",
    "\n",
    "<img src='https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_40.gif' width=500>\n",
    "\n",
    "**<center>図2.6.1-1 最適化関数</center>**\n",
    "\n",
    "最適化関数は人が調整する必要のある **ハイパーパラメータ** です。<br>\n",
    "図の通り、最適化関数にはいくつかの種類があり、正しく選択しないと **学習に悪影響**を及ぼす可能性があるので注意が必要です。\n",
    "\n",
    "> 出典: <a href='http://cs231n.github.io/neural-networks-3/#add' target='_blank'>http://cs231n.github.io/neural-networks-3/#add</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    " - 最適化関数の性質を示した文として適切なものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 一般に、最適化関数を最大化するように各層の重みを更新する。\n",
    "- 最適化関数はどれを選んでも最適化されるため、選ぶ必要はない。\n",
    "- 最適化関数は損失関数、エポック数など複数の情報を踏まえて重みの更新を行う。\n",
    "- 最適化関数には1種類しかないので、これはハイパーパラメータではない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 最適化関数は様々な要素を踏まえて重みの更新を行いますが、手法によって重みの更新の仕方が異なり、ハイパーパラメータの一種となっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 最適化関数は損失関数、エポック数など複数の情報を踏まえて重みの更新を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "S1sUOCX-l-M"
   },
   "source": [
    "## 2.7 学習率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "HkMP93IoLlM",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.7.1 学習率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>学習率</b>とは、 **各層の重みを一度にどの程度変更するかを決めるハイパーパラメータ** です。\n",
    "\n",
    "下図は、最小化を行うモデルと、 **学習率** が与える影響を図示したものです。右上の点が初期値です。\n",
    "\n",
    "<img src='https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5090_dnn/dnn_chap2_50.png'>\n",
    "\n",
    "**<center>図2.7.1-1 学習率</center>**\n",
    "\n",
    "1. 学習率が低すぎて、ほとんど更新が進んでいません。\n",
    "1. 適切な学習率により、少ない回数で値が収束しています。\n",
    "1. 収束はしますが、値が大きいため、更新に無駄があります。\n",
    "1. 学習率が高すぎて、値が発散しています。(上側に更新され、値がどんどん大きくなっています)\n",
    "\n",
    "つまり、モデルの学習を適切に行うためには、 **損失関数に対する適切な学習率を設定する必要** があるのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の3つのうち、一番精度の高い学習率を予想し、コードの一部を変更してください。\n",
    "- 学習率がモデルの学習に与える影響を確認しましょう。\n",
    "  - `funcA() lr: 0.01`\n",
    "  - `funcB() lr: 0.1`\n",
    "  - `funcC() lr: 1.0`\n",
    "- A、B、Cのなかから一つ選んで残し、それ以外の2行をコメントアウトしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "def funcA():\n",
    "    global lr\n",
    "    lr = 0.01\n",
    "\n",
    "def funcB():\n",
    "    global lr\n",
    "    lr = 0.1\n",
    "\n",
    "def funcC():\n",
    "    global lr\n",
    "    lr = 1.0\n",
    "\n",
    "# 3つのうち1つを選び、他の2行をコメントアウトして学習率を決めます。\n",
    "#---------------------------\n",
    "funcA()\n",
    "funcB()\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "sgd = optimizers.SGD(lr=lr)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全てのパターンを試してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "\n",
    "def funcA():\n",
    "    global lr\n",
    "    lr = 0.01\n",
    "\n",
    "def funcB():\n",
    "    global lr\n",
    "    lr = 0.1\n",
    "\n",
    "def funcC():\n",
    "    global lr\n",
    "    lr = 1.0\n",
    "\n",
    "# 3つのうち1つを選び、他の2行をコメントアウトして学習率を決めます。\n",
    "#---------------------------\n",
    "#funcA()\n",
    "funcB()\n",
    "#funcC()\n",
    "#---------------------------\n",
    "\n",
    "sgd = optimizers.SGD(lr=lr)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=32, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "B13IdRmWgZM"
   },
   "source": [
    "## 2.8 ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "BkXPcnIoLxz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.8.1 ミニバッチ学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "モデルに一度に入力するデータの数を<b style='color: #AA0000'>バッチサイズ</b>と呼びますが、これも **ハイパーパラメータ** のひとつです。<br>\n",
    "一度に複数のデータを渡した場合、モデルはデータごとに損失と損失関数の **勾配** を計算し、それぞれのデータの勾配の平均値をもとに1度だけ重みの更新をします。<br>\n",
    "\n",
    "このように、 **複数のデータ** で **重みの更新**を行うことで **偏ったデータの影響を減らし** 、並列計算を行うことで **計算時間を短縮** することもできます。<br>\n",
    "しかし一方で、大きな重みの更新が発生しにくくなり、 **一部のデータに最適化されてしまい、全体のデータへの最適化が行われなくなる状態（局所解）から抜け出せなくなる可能性** もあります。<br>\n",
    "\n",
    "それを回避するためには、イレギュラーなデータが多い時には **バッチサイズを大きく**する、少ないときには **バッチサイズを小さくする** といったように、バッチサイズを調整します。<br>\n",
    "\n",
    "バッチサイズを **1** に設定する学習法を <b style='color: #AA0000'>オンライン学習</b>(確率的勾配降下法)、 **全データ数** に設定する学習方法を <b style='color: #AA0000'>バッチ学習</b>(最急降下法）と呼びます。\n",
    "また、その **中間** となる少ない数に設定する学習法を <b style='color: #AA0000'>ミニバッチ学習</b>と呼びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の3つのうち一番精度の高いバッチサイズを予想し、コードの一部を変更してください。\n",
    "- バッチサイズがモデルの学習に与える影響を確認しましょう。\n",
    "  - `funcA() batch_size: 16`\n",
    "  - `funcB() batch_size: 32`\n",
    "  - `funcC() batch_size: 64`\n",
    "- A、B、Cのなかから一つ選んで残し、それ以外の2行をコメントアウトしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global batch_size\n",
    "    batch_size = 16\n",
    "\n",
    "def funcB():\n",
    "    global batch_size\n",
    "    batch_size = 32\n",
    "\n",
    "def funcC():\n",
    "    global batch_size\n",
    "    batch_size = 64\n",
    "\n",
    "# 3つのうち1つを選び、他の2行をコメントアウトしてbatch_sizeを決めてください。\n",
    "#---------------------------\n",
    "# batch_size: 16\n",
    "funcA()\n",
    "# batch_size: 32\n",
    "funcB()\n",
    "#batch_size: 64\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全てのパターン実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:1000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:1000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global batch_size\n",
    "    batch_size = 16\n",
    "\n",
    "def funcB():\n",
    "    global batch_size\n",
    "    batch_size = 32\n",
    "\n",
    "def funcC():\n",
    "    global batch_size\n",
    "    batch_size = 64\n",
    "\n",
    "# 3つのうち1つを選び、他の2行をコメントアウトしてbatch_sizeを決めてください。\n",
    "#---------------------------\n",
    "#funcA()\n",
    "#funcB()\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=3, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "BJpL_RXWlWG"
   },
   "source": [
    "## 2.9 反復学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5090,
    "exerciseId": "Sy4v9n8oUxz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 40
   },
   "source": [
    "### 2.9.1 反復学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "一般的にディープランニングでは<b style='color: #AA0000'>反復学習</b>を行い、**同じ訓練データで学習を繰り返します。**<br>\n",
    "学習の回数を **エポック数** といいますが、これもハイパーパラメータです。\n",
    "エポック数は多く設定すれば、モデルの精度が上がり続けるというものではありません。<br>\n",
    "\n",
    "適切なエポック数を設定しなかった場合、途中から精度が伸びなくなり、それだけでなく学習を繰り返すことで損失関数を最小化させようとして<a href='https://aidemy.net/courses/2010/exercises/rJ9Yn8iIlM' target='_blank'>過学習</a>を引き起こしてしまう可能性があります。<br> \n",
    "\n",
    "そのため、適切なエポック数を設定し、タイミング良く **学習を打ち切る** ことも重要なのです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の3つのうち一番精度の高いエポック数を予想し、コードの一部を変更してください。\n",
    "- エポック数がモデルの学習に与える影響を確認しましょう。\n",
    "  - `funcA() epochs: 5`\n",
    "  - `funcB() epochs: 10`\n",
    "  - `funcC() epochs: 60`\n",
    "- A、B、Cのなかから一つ選んで残し、それ以外の2行をコメントアウトしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:1500]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:6000]\n",
    "y_train = to_categorical(y_train)[:1500]\n",
    "y_test = to_categorical(y_test)[:6000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# 今回はDropoutを使いません。\n",
    "#model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global epochs\n",
    "    epochs = 5\n",
    "\n",
    "def funcB():\n",
    "    global epochs\n",
    "    epochs = 10\n",
    "\n",
    "def funcC():\n",
    "    global epochs\n",
    "    epochs = 60\n",
    "\n",
    "# 3つのうち1つを選び、他の2行をコメントアウトしてエポック数を決めてください。\n",
    "#---------------------------\n",
    "# epochs: 5\n",
    "funcA()\n",
    "# epochs: 10\n",
    "funcB()\n",
    "# epochs: 60\n",
    "funcC()\n",
    "#---------------------------\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 全てのパターンを実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:1500]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:6000]\n",
    "y_train = to_categorical(y_train)[:1500]\n",
    "y_test = to_categorical(y_test)[:6000]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "# 今回はDropoutを使いません。\n",
    "#model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "def funcA():\n",
    "    global epochs\n",
    "    epochs = 5\n",
    "\n",
    "def funcB():\n",
    "    global epochs\n",
    "    epochs = 10\n",
    "\n",
    "def funcC():\n",
    "    global epochs\n",
    "    epochs = 60\n",
    "\n",
    "# 3つのうち1つを選び、他の2行をコメントアウトしてエポック数を決めてください。\n",
    "#---------------------------\n",
    "#funcA()\n",
    "funcB()\n",
    "#funcC()\n",
    "#---------------------------\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=epochs, verbose=1, validation_data=(X_test, y_test))\n",
    "\n",
    "#acc, val_accのプロット\n",
    "plt.plot(history.history[\"acc\"], label=\"acc\", ls=\"-\", marker=\"o\")\n",
    "plt.plot(history.history[\"val_acc\"], label=\"val_acc\", ls=\"-\", marker=\"x\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "exerciseId": "sG2C2U7huf",
    "id": "movie_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 10
   },
   "source": [
    "### このコースのまとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "- このコースのまとめ\n",
    "- 次にオススメのコース\n",
    "\n",
    "\n",
    "【バーチャルYouTuber版動画】<br>\n",
    "この動画は、バーチャルYouTuber版の動画も配信されています。<br>\n",
    "https://www.youtube.com/watch?v=YWCbzfqt2GU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次の動画をみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "movielink",
    "scrolled": true
   },
   "source": [
    "https://aidemystorageprd.blob.core.windows.net/movies/5090-sG2C2U7huf.m3u8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Premium Planを受講している方は以下のリンクから受講してください。<br>\n",
    "https://aidemy.net/courses/5090/exercises/sG2C2U7huf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "chapter_exam"
   },
   "source": [
    "## 2.10 まとめ問題(提出不要)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "chapter2ではハイパーパラメータや、適切なディープニューラルネットワークの構成、調整について学習しました。<br>\n",
    "まとめ問題ではハイパーパラメータのチューニングでMNIST分類の精度向上を行い、より理解を深めましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "下記の条件にそって、MNISTの分類モデルをディープニューラルネットワークで実装しましょう。\n",
    "- テストデータによる正解率は87%以上\n",
    "- テストデータ、訓練データの正解率の差は1%未満\n",
    "- エポック数は5\n",
    "- X_train, y_train, X_test, y_test の定義文は変更不可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:10000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:10000]\n",
    "\n",
    "#---------------------------\n",
    "#  ここに条件にそったコードを書いて下さい       \n",
    "#---------------------------\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 2.1.1のコードを参考にしみましょう。コード中のハイパーパラメータを１つ変更することで正解率が87%になります。\n",
    "- 重みの初期化の仕様上、全く同じ値にはなりませんが、ほぼ毎回87%以上の正解率が出せる状態を目指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.models import Sequential, load_model\n",
    "from keras import optimizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 784)[:6000]\n",
    "X_test = X_test.reshape(X_test.shape[0], 784)[:10000]\n",
    "y_train = to_categorical(y_train)[:6000]\n",
    "y_test = to_categorical(y_test)[:10000]\n",
    "\n",
    "#---------------------------\n",
    "model = Sequential()\n",
    "model.add(Dense(256, input_dim=784))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dense(128))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.1)\n",
    "model.compile(optimizer=sgd, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=1)\n",
    "#---------------------------\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"evaluate loss: {0[0]}\\nevaluate acc: {0[1]}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "commentary"
   },
   "source": [
    "深層学習ではハイパーパラメータが数多くあるので、チューニングに大きな労力を必要とします。その際には既存のモデルなどを参考にすると良いでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
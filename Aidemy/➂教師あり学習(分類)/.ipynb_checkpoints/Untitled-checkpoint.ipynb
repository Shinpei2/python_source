{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ce926b682637>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mall_estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils.testing import all_estimators\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import MeCab\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,os.path\n",
    "import csv\n",
    "\n",
    "f = open('ファイル名', 'w')\n",
    "csv_writer = csv.writer(f, quotechar=\"'\")\n",
    "files = os.listdir('./')\n",
    "\n",
    "\n",
    "datas = []\n",
    "for filename in files:\n",
    "    if os.path.isfile(filename):\n",
    "        continue\n",
    "        \n",
    "    category = filename\n",
    "    for file in os.listdir('./'+filename):\n",
    "        path = './'+filename+'/'+file\n",
    "        r = open(path, 'r')\n",
    "        line_a = r.readlines()\n",
    "        \n",
    "        text = ''\n",
    "        for line in line_a[2:]:\n",
    "            text += line.strip()\n",
    "        r.close()\n",
    "         \n",
    "        datas.append([text, category])\n",
    "        print(text)\n",
    "csv_writer.writerows(datas)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CROSS_VALIDATION_N = 4\n",
    "\n",
    "wakati = MeCab.Tagger('-0 wakati')\n",
    "def tokenize(text):\n",
    "    '''テキストを形態解析し、形態素の配列を返す'''\n",
    "    parsed_text = wakati.parse(text)\n",
    "    word_list = parsed_text.split(' ')\n",
    "    return word_list\n",
    "\n",
    "def get_textdata_and_labels(_data):\n",
    "    \n",
    "    df = pd.DataFrame(_data)\n",
    "    df.columns = ['text', 'category']\n",
    "    \n",
    "    #テキストのBow生成\n",
    "    count_vect = CountVectorizer(analyzer=tokenize, max_df=0.5, max_features=1000)\n",
    "    bow - count_vect.fit_transform(df[\"text\"].tolist())\n",
    "    X = bow.todense()\n",
    "    \n",
    "    #ラベルデータをベクトルに変換\n",
    "    le = LabelEncoder()\n",
    "    le.fit(df['category'])\n",
    "    Y = le.transform(df['category'])\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "import csv,io\n",
    "def get_list_by_csv(file_path):\n",
    "    '''CSVから配列に変換を行う'''\n",
    "    \n",
    "    csv_reader = csv.reader(\n",
    "        io.open(file_path, \"r\", encoding = utf_8''),\n",
    "        delimiter = \",\",\n",
    "        quotechar = '\"'\n",
    "    )\n",
    "    return [row for row in csv_reader]\n",
    "\n",
    "def main():\n",
    "    _data = get_list_by_csv('corpus.csv')\n",
    "    X,Y = get_textdata_and labels(_data)\n",
    "    for (name,Estimator) in all_estimators():\n",
    "        print(name)\n",
    "        model = Estimator()\n",
    "        if 'score' not in dir(model):\n",
    "            continue;\n",
    "        try:\n",
    "                    score = cross_val_score(model, X, Y, cv=CROSS_VALIDATION_N)\n",
    "                print(scores)\n",
    "        except:\n",
    "            print(sys.exc_info())\n",
    "            pass\n",
    "        \n",
    "if__name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

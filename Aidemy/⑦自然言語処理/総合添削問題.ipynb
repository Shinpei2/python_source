{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初めの一回だけこのセルを実行してください、データセットをダウンロードして展開します\n",
    "# 一回実行すれば、データセットはダウンロードされたままなので、再起動後等再び実行する必要はありません\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# URLを指定\n",
    "url = \"https://storage.googleapis.com/tutor-contents-dataset/5050_nlp_data.zip\"\n",
    "save_name = url.split('/')[-1]\n",
    "\n",
    "# ダウンロードする\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# ファイルへ保存\n",
    "with open(save_name, mode='wb') as f:\n",
    "    f.write(mem)\n",
    "\n",
    "# zipファイルをカレントディレクトリに展開する\n",
    "zfile = zipfile.ZipFile(save_name)\n",
    "zfile.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "final_exam"
   },
   "source": [
    "# 総合添削問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 学習データに出現する単語をテキストの特徴量とし、ランダムフォレスト、ナイーブベイズの2つの分類器を学習させ、livedoor newsコーパスでの精度を評価してください。\n",
    "- また、テキストの特徴量を名詞、動詞、形容詞、形容動詞のみに限定した時のモデルの精度を同様に評価してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- ナイーブベイズ分析には、`sklearn.naive_bayes.MultinomialNB()`を使用してください\n",
    "<br>[参考]　scikit-learn公式ドキュメント https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "■添削コメント\n",
    "添削問題お疲れ様です\n",
    "コード確認いたしました\n",
    "\n",
    "コード・出力ともにバッチリです。\n",
    "\n",
    "求められている\n",
    "・単語のベクトル化\n",
    "・モデルの構築\n",
    "・品詞の取り出し\n",
    "\n",
    "などの実装も全てかけていて、素晴らしいです。\n",
    "\n",
    "品詞の取り出しでは、\n",
    "条件分岐を用いて、それぞれの品詞に対する語彙\n",
    "を追加できていますね。\n",
    "(ここが書けていない生徒さんが多いので、ご自身で工夫して書かれていると思います)\n",
    "\n",
    "引き続き頑張りましょう\n",
    "\n",
    "参考リンク:\n",
    "https://www.rondhuit.com/download.html\n",
    "http://blog.livedoor.jp/newscorpus_download/archives/2749191.html\n",
    "\n",
    "補足 :\n",
    "今回はlivedoorのコーパスを用いました。\n",
    "自然言語処理のタスクでは、大規模なコーパスが必要になります\n",
    "そのときは、青空文庫やwkipediaなどを使ってみてくださいね\n",
    "\n",
    "https://arakan-pgm-ai.hatenablog.com/entry/2018/02/12/060000\n",
    "https://qiita.com/yura/items/6c1481ca652d3d131e47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        'dokujo-tsushin': 1,\n",
    "        'it-life-hack': 2,\n",
    "        'kaden-channel': 3,\n",
    "        'livedoor-homme': 4,\n",
    "        'movie-enter': 5,\n",
    "        'peachy': 6,\n",
    "        'smax': 7,\n",
    "        'sports-watch': 8,\n",
    "        'topic-news': 9\n",
    "    }\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = ''\n",
    "        for file in files:\n",
    "            with open(file, 'r', errors='ignore') as f:\n",
    "                lines = f.read().splitlines()\n",
    "\n",
    "                # 1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]\n",
    "                datetime = lines[1]\n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# indices は0からドキュメントの数までの整数をランダムに並べ替えた配列\n",
    "random.seed()\n",
    "indices = list(range(len(docs)))\n",
    "# 9割をトレーニングデータとする\n",
    "separate_num = int(len(docs) * 0.9)\n",
    "\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_data = [docs[i] for i in indices[0:separate_num]]\n",
    "train_labels = [labels[i] for i in indices[0:separate_num]]\n",
    "test_data = [docs[i] for i in indices[separate_num:]]\n",
    "test_labels = [labels[i] for i in indices[separate_num:]]\n",
    "\n",
    "# Tf-idfを用いてtrain_dataをベクトル化してください\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# ナイーブベイズを用いて分類をおこなってください。\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "# ランダムフォレストを用いて分類をおこなってください\n",
    "clf2 = RandomForestClassifier(n_estimators=2)\n",
    "clf2.fit(train_matrix, train_labels)\n",
    "\n",
    "# テストデータを変換\n",
    "test_matrix = vectorizer.transform(test_data)\n",
    "\n",
    "# 分類結果を表示\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))\n",
    "print(clf2.score(train_matrix, train_labels))\n",
    "print(clf2.score(test_matrix, test_labels))\n",
    "\n",
    "\n",
    "# 単語の抽出\n",
    "def tokenize(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(','.join(text))\n",
    "    noun = []\n",
    "    for token in tokens:\n",
    "        # 「名詞」「動詞」「形容詞」「形容動詞」を取り出してください\n",
    "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    "        if part_of_speech == \"名詞\" or part_of_speech == \"動詞\" or part_of_speech == \"形容詞\" or part_of_speech == \"形容動詞\":\n",
    "            noun.append(token.surface)\n",
    "    return noun\n",
    "\n",
    "\n",
    "# 単語の抽出して学習\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize(docs))\n",
    "train_matrix = vectorizer.fit_transform(train_data)\n",
    "test_matrix = vectorizer.transform(test_data)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "clf2.fit(train_matrix, train_labels)\n",
    "\n",
    "# 結果を表示\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    print(clf.score(train_matrix, train_labels))\n",
    "    print(clf.score(test_matrix, test_labels))\n",
    "    print(clf2.score(train_matrix, train_labels))\n",
    "    print(clf2.score(test_matrix, test_labels))\n",
    "    elapsed_time = time.time() - start\n",
    "    print(\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        'dokujo-tsushin': 1,\n",
    "        'it-life-hack': 2,\n",
    "        'kaden-channel': 3,\n",
    "        'livedoor-homme': 4,\n",
    "        'movie-enter': 5,\n",
    "        'peachy': 6,\n",
    "        'smax': 7,\n",
    "        'sports-watch': 8,\n",
    "        'topic-news': 9\n",
    "    }\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = ''\n",
    "        for file in files:\n",
    "            with open(file, 'r', errors='ignore') as f:\n",
    "                lines = f.read().splitlines()\n",
    "\n",
    "                # 1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]\n",
    "                datetime = lines[1]\n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# indices は0からドキュメントの数までの整数をランダムに並べ替えた配列\n",
    "random.seed()\n",
    "indices = list(range(len(docs)))\n",
    "# 9割をトレーニングデータとする\n",
    "separate_num = int(len(docs) * 0.9)\n",
    "\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_data = [docs[i] for i in indices[0:separate_num]]\n",
    "train_labels = [labels[i] for i in indices[0:separate_num]]\n",
    "test_data = [docs[i] for i in indices[separate_num:]]\n",
    "test_labels = [labels[i] for i in indices[separate_num:]]\n",
    "\n",
    "# Tf-idfを用いてtrain_dataをベクトル化してください\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(train_data)\n",
    "\n",
    "# ナイーブベイズを用いて分類をおこなってください。\n",
    "clf = MultinomialNB()\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "# ランダムフォレストを用いて分類をおこなってください\n",
    "clf2 = RandomForestClassifier(n_estimators=100)\n",
    "clf2.fit(train_matrix, train_labels)\n",
    "\n",
    "# テストデータを変換\n",
    "test_matrix = vectorizer.transform(test_data)\n",
    "\n",
    "# 分類結果を表示\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))\n",
    "print(clf2.score(train_matrix, train_labels))\n",
    "print(clf2.score(test_matrix, test_labels))\n",
    "\n",
    "\n",
    "# 単語の抽出\n",
    "def tokenize(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(','.join(text))\n",
    "    noun = []\n",
    "    for token in tokens:\n",
    "        # 「名詞」「動詞」「形容詞」「形容動詞」を取り出してください\n",
    "        partOfSpeech = token.part_of_speech.split(',')[0]\n",
    "\n",
    "        if partOfSpeech == '名詞':\n",
    "            noun.append(token.surface)\n",
    "        if partOfSpeech == '動詞':\n",
    "            noun.append(token.surface)\n",
    "        if partOfSpeech == '形容詞':\n",
    "            noun.append(token.surface)\n",
    "        if partOfSpeech == '形容動詞':\n",
    "            noun.append(token.surface)\n",
    "    return noun\n",
    "\n",
    "\n",
    "# 単語の抽出して学習\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize(docs))\n",
    "train_matrix = vectorizer.fit_transform(train_data)\n",
    "test_matrix = vectorizer.transform(test_data)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "clf2.fit(train_matrix, train_labels)\n",
    "\n",
    "# 結果を表示\n",
    "if __name__ == '__main__':\n",
    "    start = time.time()\n",
    "    print(clf.score(train_matrix, train_labels))\n",
    "    print(clf.score(test_matrix, test_labels))\n",
    "    print(clf2.score(train_matrix, train_labels))\n",
    "    print(clf2.score(test_matrix, test_labels))\n",
    "    elapsed_time = time.time() - start\n",
    "    print(\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

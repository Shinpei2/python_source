{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.show()で可視化されない人はこのセルを実行してください。\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初めの一回だけこのセルを実行してください、データセットをダウンロードして展開します\n",
    "# 一回実行すれば、データセットはダウンロードされたままなので、再起動後等再び実行する必要はありません\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# URLを指定\n",
    "url = \"https://storage.googleapis.com/tutor-contents-dataset/5050_nlp_data.zip\"\n",
    "save_name = url.split('/')[-1]\n",
    "\n",
    "# ダウンロードする\n",
    "mem = urllib.request.urlopen(url).read()\n",
    "\n",
    "# ファイルへ保存\n",
    "with open(save_name, mode='wb') as f:\n",
    "    f.write(mem)\n",
    "\n",
    "# zipファイルをカレントディレクトリに展開する\n",
    "zfile = zipfile.ZipFile(save_name)\n",
    "zfile.extractall('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "chapterId": "B10Nu0mWl-M",
    "id": "chapter_name"
   },
   "source": [
    "# 自然言語のベクトル表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "table"
   },
   "source": [
    "- **[2.1 文書のベクトル表現](#2.1-文書のベクトル表現)**\n",
    "    - **[2.1.1 文書のベクトル表現](#2.1.1-文書のベクトル表現)**\n",
    "    - **[2.1.2 BOW_カウント表現](#2.1.2-BOW_カウント表現)**\n",
    "    - **[2.1.3 BOW_tf-idfによる重み付け（理論）](#2.1.3-BOW_tf-idfによる重み付け（理論）)**\n",
    "    - **[2.1.4 BOW_tf-idfによる重み付け（実装）](#2.1.4-BOW_tf-idfによる重み付け（実装）)**\n",
    "    - **[2.1.5 cos類似度](#2.1.5-cos類似度)**\n",
    "<br><br>\n",
    "- **[2.2 単語のベクトル表現](#2.2-単語のベクトル表現)**\n",
    "    - **[2.2.1 Word2Vec](#2.2.1-Word2Vec)**\n",
    "    - **[2.2.2 globモジュール](#2.2.2-globモジュール)**\n",
    "    - **[2.2.3 with文](#2.2.3-with文)**\n",
    "    - **[2.2.4 コーパスの取り出し](#2.2.4-コーパスの取り出し)**\n",
    "    - **[2.2.5 Word2Vec（実装）](#2.2.5-Word2Vec（実装）)**\n",
    "<br><br>\n",
    "- **[2.3 Doc2Vec](#2.3-Doc2Vec)**\n",
    "    - **[2.3.1 Doc2Vec（1）](#2.3.1-Doc2Vec（1）)**\n",
    "    - **[2.3.2 Doc2Vec（2）](#2.3.2-Doc2Vec（2）)**\n",
    "<br><br>\n",
    "- **[2.4 日本語テキストの分類](#2.4-日本語テキストの分類)**\n",
    "    - **[2.4.1 日本語テキストの分類](#2.4.1-日本語テキストの分類)**\n",
    "    - **[2.4.2 fit関数](#2.4.2-fit関数)**\n",
    "    - **[2.4.3 実装](#2.4.3-実装)**\n",
    "    - **[2.4.4 精度を上げる](#2.4.4-精度を上げる)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "HkkH_07bg-M"
   },
   "source": [
    "## 2.1 文書のベクトル表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "SyH753LsLlM",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.1.1 文書のベクトル表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "Chapter1では、主に文書や文章の扱いに対して説明しました。Chapter2では、文章や単語同士の関係を解析していきます。\n",
    "\n",
    "<b style='color: #AA0000'>文書のベクトル表現</b>とは、 **文書中に単語がどのように分布しているかをベクトルとして表現すること**です。<br>\n",
    "例えば、「トマトときゅうりだとトマトが好き」という文は以下のようなベクトル表現に変換することができます。<br>\n",
    "`（が、きゅうり、好き、だと、と、トマト） = (1, 1, 1, 1, 1, 2)`<br>\n",
    "各単語の出現回数は表現されていますが、どこに出現したかの情報は失われています。<br>\n",
    "つまり、構造や語順の情報が失われています。このようなベクトル表現方法を <b style='color: #AA0000'>Bag of Words(BOW)</b>と呼びます。<br>\n",
    "\n",
    "ベクトル表現に変換する方法には、代表的なものが3つあります。\n",
    "> - **カウント表現**：先ほどの例のように、文書中の各単語の出現数に着目する方法\n",
    "> - **バイナリ表現**：出現頻度を気にせず、文章中に各単語が出現したかどうかのみに着目する方法\n",
    "> - **tf-idf表現**：tf-idfという手法で計算された、文章中の各単語の重み情報を扱う方法\n",
    "\n",
    "一般的には、 **tf-idf** が使われていますが、 **文章数が多い場合においては計算に時間がかかる** ため、 **バイナリ表現やカウント表現** を用います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下の選択肢から正しいものを選択してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- ベクトル表現化の方法はただ一つしか存在しない。\n",
    "- カウント表現とは、単語が出現したかしていないかのみに着目している。\n",
    "- どの場合でも、tf-idf表現を用いれば良い。\n",
    "- 場合に合わせて一番適した表現を用いるのが良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- それぞれのベクトル表現に適材適所があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 場合に合わせて一番適した表現を用いるのが良い。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "rkUQc38sIxf",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.1.2 BOW カウント表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>カウント表現</b>では文書中の各単語の出現回数をカウントすることによって、文書をベクトルに変換していきます。\n",
    "1. John likes to watch movies. Mary likes movies too.\n",
    "2. John also likes to watch football games.  \n",
    "\n",
    "上の２つの文があった時、この2文のBag of Wordsは、文書中にある単語の出現回数を要素として使用したベクトルになります。\n",
    "```python\n",
    "[\"John\",\"likes\",\"to\",\"watch\",\"movies\",\"Mary\",\"too\",\"also\",\"football\", \"games\"]  \n",
    "```\n",
    "上記の単語が出現する回数は\n",
    "```python\n",
    "1. [1, 2, 1, 1, 2, 1, 1, 0, 0, 0]\n",
    "2. [1, 1, 1, 1, 0, 0, 0, 1, 1, 1]\n",
    "```\n",
    "となっています。\n",
    "\n",
    "これは **文章の特徴** を表しています。\n",
    "\n",
    "Pythonでは、 **gensim** という主にテキスト解析を対象とした機械学習ライブラリを用いることで、自動的に計算をすることが可能です。<br>\n",
    "まず、 **`dictionary = gensim.corpora.Dictionary(文書)`** により、文書に登場する単語の辞書`dictionary`をあらかじめ作成します。<br>\n",
    "**`dictionary.doc2bow(分かち書きされた文章)`** でBag of Wordsを作成することができ、出力結果としては **`(id, 出現回数)`のリスト** が作成されます。<br>\n",
    "**`dictionary.token2id`** で各単語のid番号を取得できます。\n",
    "\n",
    "```python\n",
    "from gensim import corpora\n",
    "\n",
    "# 文書毎の単語のリスト(documents)から単語と出現回数の対応辞書を作成\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# Bag of Wordsの作成\n",
    "bow_corpus = [dictionary.doc2bow(d) for d in documents]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "> すもももももももものうち<br>\n",
    "> 料理も景色もすばらしい<br>\n",
    "> 私の趣味は写真撮影です  \n",
    "\n",
    "- 上記の3つの文をjanomeを使い分かち書きをした後、**カウント表現** のBag of Wordsを作り、表示してください。\n",
    "- また、各単語のidを表示してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "text1 = \"すもももももももものうち\"\n",
    "text2 = \"料理も景色もすばらしい\"\n",
    "text3 = \"私の趣味は写真撮影です\"\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens1 = t.tokenize(text1, wakati=True)\n",
    "tokens2 = t.tokenize(text2, wakati=True)\n",
    "tokens3 = t.tokenize(text3, wakati=True)\n",
    "\n",
    "documents = [tokens1, tokens2, tokens3]\n",
    "# corporaを使い単語辞書を作成してください。\n",
    "dictionary =\n",
    "\n",
    "# 各単語のidを表示してください\n",
    "\n",
    "\n",
    "# Bag of Wordsの作成してください\n",
    "bow_corpus =\n",
    "\n",
    "# (id, 出現回数)のリストが出力されます。\n",
    "print(bow_corpus)\n",
    "\n",
    "print()\n",
    "# bow_corpusの内容をわかりやすく出力する\n",
    "texts = [text1, text2, text3]\n",
    "for i in range(len(bow_corpus)):\n",
    "    print(texts[i])\n",
    "    for j in range(len(bow_corpus[i])):\n",
    "        index = bow_corpus[i][j][0]\n",
    "        num = bow_corpus[i][j][1]\n",
    "        print(\"\\\"\", dictionary[index], \"\\\" が \" ,num, \"回\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- Bag of Wordsを作成するためにはgensim.corpora(文書)で出現回数の対応辞書を作成します。\n",
    "- doc2bow()を利用してBag of Wordsを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "text1 = \"すもももももももものうち\"\n",
    "text2 = \"料理も景色もすばらしい\"\n",
    "text3 = \"私の趣味は写真撮影です\"\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens1 = t.tokenize(text1, wakati=True)\n",
    "tokens2 = t.tokenize(text2, wakati=True)\n",
    "tokens3 = t.tokenize(text3, wakati=True)\n",
    "\n",
    "documents = [tokens1, tokens2, tokens3]\n",
    "# corporaを使い単語辞書を作成\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# 各単語のidを表示してください\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# Bag of Wordsの作成\n",
    "bow_corpus = [dictionary.doc2bow(d) for d in documents]\n",
    "\n",
    "# (id, 出現回数)のリスト\n",
    "print(bow_corpus)\n",
    "\n",
    "print()\n",
    "# bow_corpusの内容をわかりやすく出力する\n",
    "texts = [text1, text2, text3]\n",
    "for i in range(len(bow_corpus)):\n",
    "    print(texts[i])\n",
    "    for j in range(len(bow_corpus[i])):\n",
    "        index = bow_corpus[i][j][0]\n",
    "        num = bow_corpus[i][j][1]\n",
    "        print(\"\\\"\", dictionary[index], \"\\\" が \" ,num, \"回\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "SyPQqn8jIeG",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.1.3 BOW tf-idfによる重み付け（理論）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "「BOW_カウント表現」で行ったカウント表現では、**文章を特徴付ける単語の出現回数を特徴量**として扱いました。  \n",
    "\n",
    "\n",
    " <b style='color: #AA0000'>tf-idf</b>は単語の出現頻度である **$tf$ (Term frequency)** と、その単語がどれだけ珍しいか（希少性）をしめす逆文書頻度 **$idf$(Inverse Document Frequency)** の **積**で 表されます。  \n",
    "$tf$,$idf$は下記の式で定義されます。  \n",
    "\n",
    "$$\n",
    "tf_{i,j} = \\frac{n_{i,j}}{\\displaystyle\\sum_{k}{n_{k,j}}}\n",
    "$$\n",
    "$$ \n",
    "idf_{i} = log\\frac{|D + 1|}{|df_{i} + 1|}\n",
    "$$\n",
    "$$ \n",
    "tf idf_{i,j} = tf_{i,j} \\cdot idf_{i} \n",
    "$$\n",
    "\n",
    "\n",
    " - $n_{i,j}$は文書$d_{j}$における単語$t_{i}$の出現回数\n",
    " - $\\Sigma_{k}{n_{k,j}}$は文書$d_{j}$におけるすべての単語の出現回数の和\n",
    " - ${|D|}$は総文書数\n",
    " - ${|df_{i}|}$は単語${t_{i}}$を含む文書数\n",
    "  \n",
    "$idf_{i}$は単語${t_{i}}$を含む文書数が少ないほど大きな値となるので **多くの文書に出現する語（一般的な語）の重要度を下げ、特定の文書にしか出現しない単語の重要度を上げる役割を果たす** ことが式からわかります。  \n",
    "これによって「です」「ます」などの値が小さくなり、正しく重要度を設定することができます。\n",
    "\n",
    "つまり、<b style='color: #AA0000'>tf-idf</b>では、特定の文書中にのみ多く出現し、他の文書ではあまり出現しないような、 **出現の分布に偏りのある単語の重要度が高く** なります。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下の文章の空欄に入る単語の組み合わせを選んでください。\n",
    "- `tf-idf`では、「」単語ほど価値は高くなります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 「様々な文書で頻出する」\n",
    "- 「特定の文書のみに頻出する」\n",
    "- 「どの文書においても希少な」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- tf-idfではその文章に特有の単語であるほど高い値を返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 「特定の文書のみに頻出する」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "HydX93IoUxz",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.1.4 BOW tf-idfによる重み付け（実装）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "実際にtf-idfを実装していきたいと思います。 <br>\n",
    "scikit-learnが提供しているパッケージ<b style='color: #AA0000'>TfidfVectorizer</b>を用いて実装します。<br>\n",
    "\n",
    "TfidfVectorizerは、「BOW　tf-idfによる重み付け（理論）」で説明した式を少し改良した形で実装されていますが、本質的な部分は同じです。\n",
    "\n",
    "`TfidfVectorizer`を用いた文書のベクトル表現化の実装は以下のようになります。<br>\n",
    "\n",
    "```python \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 表示するときに有効数字２桁で表示する\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# 分かち書きされた文書\n",
    "docs = np.array([\n",
    "    \"白　黒　赤\", \n",
    "    \"白　白　黒\", \n",
    "    \"赤　黒\"\n",
    "])\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "\n",
    "# 列要素を取得します\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "# tf-idf値を格納した行列を取得します\n",
    "print(vecs.toarray())\n",
    "#出力結果\n",
    "['白', '赤', '黒']\n",
    "[[ 0.62  0.62  0.48]\n",
    "[ 0.93  0.    0.36]\n",
    "[ 0.    0.79  0.61]]\n",
    "```\n",
    "\n",
    "`vecs.toarray()`のn行目が、もとの文書`docs`のn番目のベクトル表現に対応しています。\n",
    "そして、`vecs.toarray()`のn列目が、全ての単語のベクトル表現に対応しています。\n",
    "\n",
    "<img src=\"https://aidemyexstorage.blob.core.windows.net/aidemycontents/1552894106845624.png\t\">\n",
    "\n",
    "\n",
    "tf-idfベクトルについて詳しく知りたい方はAidemyが書いた<a href=\"https://qiita.com/MasatoTsutsumi/items/5b0a140b1ecbdd0396e1\">こちらの記事</a>をご覧になって下さい\n",
    "\n",
    "コードの補足をします。<br>\n",
    "\n",
    "**`vectorizer = TfidfVectorizer()`** で、ベクトル表現化を行う変換器を生成します。\n",
    "**`use_idf=False`** にすると、tfのみの重み付けになります。<br>\n",
    "`TfidfVectorizer`は、デフォルトで1文字の文字や文字列をトークンとして扱わない仕様になっているため、<br>\n",
    "引数に **`token_pattern=\"(?u)\\\\b\\\\w+\\\\b\"`** を追加することで除外しないようにする必要があります。<br>\n",
    "`\"(?u)\\\\b\\\\w+\\\\b\"`は、「1文字以上の任意の文字列」を表す正規表現ですが、深く理解する必要はありません。（正確には正規表現にエスケープシーケンスをつけたものです。）\n",
    "\n",
    "\n",
    "**`vectorizer.fit_transform()`** で、文書をベクトルに変換します。引数には、 **空白文字によって分割された（分かち書きされた）文書からなる配列** を与えます。\n",
    "`toarray`によって出力をNumpyのndarray配列に変換できます。\n",
    "\n",
    "**`np.set_printoptions()`** は、numpy配列の表示のフォーマットを定める関数であり、`precision`で有効数字を指定することができます。<br>\n",
    "今回の例だと、有効数字二桁で表示されます。<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 説明文の例を参考に`「\"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"」`という文を`TfidfVectorizer`を使ってベクトル表現に変換してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"\n",
    "])\n",
    "\n",
    "# ベクトル表現に変換してください。\n",
    "vectorizer = \n",
    "vecs = \n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vecs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- ベクトル表現化を行う変換器` TfidfVectorizer() `を生成して、`.fit_transform()`メソッドで、文書をベクトルに変換しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"\n",
    "])\n",
    "\n",
    "# ベクトル表現に変換してください。\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vecs.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "BJK793LjIlf",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.1.5 cos類似度 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "これまでに文書を定量的に判断するために、文書のベクトル化をしてきました。<br>\n",
    "そのベクトルを比較することにより、文書同士の類似度を解析することができます。<br>\n",
    "ベクトルとベクトルがどれだけ近いものか示してくれるものに <b style='color: #AA0000'>cos類似度</b>というものがあります。\n",
    "\n",
    "cos類似度は下記の数式で表され、ベクトルのなす角のコサイン(0~1)を表します。\n",
    "そのためcos類似度は、二つのベクトルの方向が近いときに高い値を、反対の方向に向いている時に小さい値をとります。<br>\n",
    " **「1に近い場合は似ていて、0に近いときは似ていない」** ということをしっかりおさえておきましょう。\n",
    "\n",
    "\n",
    " $$ cos(\\vec{q},\\vec{d})=\\frac{\\vec{q}\\vec{d}}{|\\vec{q}||\\vec{d}|}=\\frac{\\vec{q}}{|\\vec{q}|}\\frac{\\vec{d}}{|\\vec{d}|}=\\frac{\\Sigma_{i=1}{q_{i}}{d_{i}}}{{\\sqrt{\\Sigma_{i=1}{q_{i}}^2}}{\\sqrt{\\Sigma_{i=1}{d_{i}}^2}}} $$\n",
    "\n",
    "実装すると以下のようになります。<br>\n",
    "`np.dot()`は内積を表し、`np.linalg.norm`はベクトルのノルム（ベクトルの長さ）を表しています。\n",
    "\n",
    "```python \n",
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return cos_sim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- `\"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"`という文の類似度を計算して出力してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"\n",
    "])\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "vecs = vecs.toarray()\n",
    "\n",
    "# cos類似度を求める関数を定義してください。\n",
    "def cosine_similarity(v1, v2):\n",
    "    cos_sim = \n",
    "    return\n",
    "\n",
    "\n",
    "# 類似度を比較してみましょう。\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[0], vecs[1]))\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[0], vecs[2]))\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[1], vecs[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `np.dot()`で二つのベクトルの内積を計算することができ、`np.linalg.norm`でベクトルのノルム（ベクトルの長さ）を計算することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"\n",
    "])\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "vecs = vecs.toarray()\n",
    "\n",
    "# cos類似度を求める関数を定義してください。\n",
    "def cosine_similarity(v1, v2):\n",
    "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return cos_sim\n",
    "\n",
    "# 類似度を比較してみましょう。\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[0], vecs[1]))\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[0], vecs[2]))\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[1], vecs[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "S1gSuCQbeZG"
   },
   "source": [
    "## 2.2 単語のベクトル表現"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "Sy5XchIj8lz",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.1 Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "前節では文書をベクトル表現として表していましたが、今回は単語をベクトル化します。<br>\n",
    "単語をベクトルで表現すると、単語の意味の近さの数値化や同義語の探索などが行えます。<br>\n",
    "\n",
    "最近の研究により、新しく生まれた単語をベクトル化するツールに <b style='color: #AA0000'>Word2Vec</b>というものがあります。  \n",
    "簡潔に述べると **単語の意味や文法を捉えるために単語をベクトル表現化して次元を圧縮するツール** です。  \n",
    "日本人が日常的に使う語彙数は数万から数十万といわれていますが、Word2Vecでは各単語を200次元程度のベクトルとして表現できます。 \n",
    "\n",
    "Word2Vecを用いると単語と単語の関係性を簡単に表現でき、<br>\n",
    "`「王様」 - 「男」+ 「女」 = 「女王」` <br>\n",
    "`「パリ」 - 「フランス」 + 「日本」 = 「東京」` <br>\n",
    "のような **単語同士の** 演算も可能となります。  \n",
    " \n",
    "ここでは **Word2Vec** に「livedoor newsコーパス」というニュース記事の文書データを与え、単語の関係性を学習させていきます。\n",
    "\n",
    "これから、 **Word2Vec** を用いて\"男\"という単語と関連性が高い文字列を調べていきます。<br>\n",
    "フローは以下のようになります。対応するセッション名も同時に記しています。<br>\n",
    "\n",
    "> 1. ニュース記事をテキストコーパスとして取り出し、文章とカテゴリーに分ける。 : 「globモジュール」、「with文」、「コーパスの呼び出し」\n",
    "> 2. 取り出した文書を品詞ごとに分割し、リストにする。: 「janome(3)」\n",
    "> 3. Word2Vecでモデルを生成。: 「Word2Vec（実装）」\n",
    "> 4. 男との関連性が高い語を調べる。: 「Word2Vec（実装）」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下の空欄に当てはまる単語の組み合わせを選択してください。\n",
    "- 文章をベクトルで表現する方法を「」と言います。また単語をベクトルで表現するツールとして「」があげられます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 「カウント表現」、「Word2Vec」\n",
    "- 「BOW」、「CBOW」\n",
    "- 「BOW」、「Word2Vec」\n",
    "- 「tf-idf」、「CBOW」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 以下を参考にしましょう。\n",
    ">1. カウント表現：文書中の各単語の出現数\n",
    ">2. BOW：文書のベクトル表現\n",
    ">3. Word2Vec：単語をベクトル化するツール\n",
    ">4. tf-idf：文章中の各単語の重み情報を扱う方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "「BOW」、「Word2Vec」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "BJsX5hLiUlG",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.2 globモジュール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>globモジュール</b>はファイルやディレクトリを操作するときに便利なモジュールであり、正規表現を用いてパスを指定できます。\n",
    "\n",
    "> - **ファイル**：文書、写真、音楽など、ユーザーが操作・管理する情報の最小単位。\n",
    "> - **ディレクトリ**：ファイルをまとめる入れ物のこと。\n",
    "> - **パス**：コンピュータ上でファイルやディレクトリの場所のこと。\n",
    "\n",
    "globモジュールが **osモジュール** （基本的にファイルやディレクトリを操作するときに用いるモジュール）と異なる点は、<br>\n",
    "**特殊な文字や文字列を用いて賢くファイルを検索できる点** です。<br>\n",
    "例えば、アスタリスク **`*`** を用いて以下の例のように記述すると、 testディレクトリにあるtxtファイルを全て表示させることができます。  \n",
    "```python\n",
    "import glob\n",
    "\n",
    "lis = glob.glob(\"test/*.txt\")\n",
    "print(lis)\n",
    "# 出力結果\n",
    "[\"test/sample.txt\", \"test/sample1.txt\", \"test/sample2.txt\"]\n",
    "```\n",
    "\n",
    "また、以下の例のようにして特殊な文字列を用いることができます。この例では、testディレクトリ以下のsample(数字).txtファイルを全て表示させることができます。\n",
    "```python\n",
    "import glob\n",
    "\n",
    "lis = glob.glob(\"test/sample[0-9].txt\")\n",
    "print(lis)\n",
    "# 出力結果\n",
    "[\"test/sample1.txt\", \"test/sample2.txt\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- Aidemyが提供するコードの実行環境には、livedoor newsコーパスの大量のニュース記事データがカテゴリーごとに保存された、textディレクトリが用意されています。\n",
    "globモジュールを用いて`text/sports-watch`ディレクトリの中にあるテキストファイルのファイル名のリストを取得し、リストのはじめの3つの要素を出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# text/sports-watchの中にあるファイルを表示してください\n",
    "lis = \n",
    "print(lis[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- パス以下の全てのテキストファイルを表示させたいときは、`glob.glob(\"パス/*.txt\")`としましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# text/sports-watchの中にあるファイルを表示してください\n",
    "lis = glob.glob(\"text/sports-watch/*.txt\")\n",
    "print(lis[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "rknmc2IoUgM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.2.3 with文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "通常ファイルを読み込む際には、`open()`でファイルを開き、`read()`等でファイルを読み込み、`close()`でファイルを閉じます。\n",
    "\n",
    "下の例の`open()`の第二引数`\"r\"`はファイルを開くときのモードを指定するもので、この場合だとreadの意で読み込み専用であることを示しています。他にも、例えば書き込み専用ならwriteの意で`\"w\"`と指定したりします。\n",
    "```python\n",
    "f = open(\"a.text\", \"r\", encoding=\"utf-8\")\n",
    "data = f.read()\n",
    "f.close()\n",
    "```\n",
    "しかしこの表記の仕方だと、`close()`を書き忘れてしまったり、途中でエラーが発生しファイルが閉じられず、メモリが無駄に占有される恐れがあります。\n",
    "\n",
    "そこで代わりに <b style='color: #AA0000'>with文</b>を用います。 **with文**　を用いると、ファイルが自動的に`close()`されたり、ファイルを開いている際にエラーが発生しても適切な例外処理が自動的に行われるので、とても便利です。\n",
    "\n",
    " **with文を用いた場合** 、以下のようにしてファイルを開きます。<br>\n",
    " この例では、最もポピュラーな **文字コード** `UTF-8`でファイルを出力しています。文字コードとは、コンピュータ上で文字を扱うために個々の文字に割り振られた番号のことです。 \n",
    "```python\n",
    "with open(\"a.text\", \"r\", encoding=\"utf-8\") as f:\n",
    "```\n",
    "with文中で、例えば\n",
    "```python\n",
    "data = f.read()\n",
    "```\n",
    "等でファイルの読み込みを行います。`read()`を使うと、ファイル内のデータ全てを文字列として読み込みます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- `with`文を用いて`text/sports-watch`上にある`LICENSE.txt`を開き、出力してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "# with文を用いてtext/sports-watch上にあるLICENSE.txtを開き、出力してください\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "```python\n",
    "with open(\"a.text\", \"r\", encoding=\"utf-8\") as f:\n",
    "```\n",
    "でテキストを開けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "# with文を用いてtext/sports-watch上にあるLICENSE.txtを開き、出力してください\n",
    "with open(\"text/sports-watch/LICENSE.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "r1TQ92LjUeM",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.2.4 コーパスの取り出し"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>コーパス</b>とは、 **文書または音声データにある種の情報を与えたデータ** のことでした。\n",
    "\n",
    "**livedoor newsコーパス** とは、ダウンロード元によると以下のようなデータとされています。\n",
    "```python\n",
    "NHN Japan株式会社が運営する「livedoor ニュース」のうち、クリエイティブ・コモンズライセンスが適用されるニュース記事を収集し、可能な限りHTMLタグを取り除いて作成してあるものです。\n",
    "通常必要なデータの整形作業が必要ないものです。\n",
    "```\n",
    "\n",
    "Aidemyが提供するコードの実行環境には、livedoor newsコーパスの大量のニュース記事データが保存された、5050_nlp_dataディレクトリが用意されています。5050_nlp_dataディレクトリの中には9つのカテゴリーのディレクトリがあり、それぞれのディレクトリの中にニュース記事データのtxtファイルが入っています。以下のコードでtextディレクトリや、カテゴリーごとのディレクトリの内容を確認できます。\n",
    "```python\n",
    "glob.glob(\"./5050_nlp_data/*\")\n",
    "glob.glob(\"./text/sports-watch/*\")\n",
    "```\n",
    "\n",
    "今回は、livedoor newsコーパスをテキストコーパスとして取り出し、文章とカテゴリーに分けて利用します。\n",
    "以下のように表記することで、コーパスを取り出して分類することができます。<br>\n",
    "一見複雑ですが、内容はとても単純です。\n",
    "\n",
    "`splitlines()`は文字列を改行部分で分解し、各行からなるリストを返す組み込み関数です。\n",
    "```python\n",
    "import glob\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = [] # 全ての記事の文章をここに格納します。\n",
    "    labels = [] # docsに格納される記事の1〜9のカテゴリーを、ラベルとして扱います。\n",
    "\n",
    "    # 全てのカテゴリーのディレクトリについて実行します。\n",
    "    for c_name, c_id in category.items():\n",
    "        # {c_name}にcategory.items()から取得したカテゴリー名c_nameをformatメソッドを用いて埋め込みます。\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "        # カテゴリーに属するファイル数（記事の数）を表示します。\n",
    "        print(\"category: \", c_name, \", \",  len(files))\n",
    "        # 各記事について、URL、 日付、タイトル、 本文の情報を以下のようにして取得します。\n",
    "        for file in files:\n",
    "            # with文を用いるため、close()は不要です。\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                # 改行文字で分割\n",
    "                lines = f.read().splitlines()\n",
    "                # 分割すると0番目にurl, 1番目に日付、2番目にタイトル、3番目以降に記事本文が記載されています。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                # 記事中の本文を1行にまとめてしまいます。\n",
    "                body = \"\".join(lines[3:])\n",
    "                # タイトルと本文をまとめてしまいます。\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "    \n",
    "# 全ての記事の文章データとそのラベル（カテゴリー）を取得します。\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 説明文のコードを写して実行しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "    \n",
    "    # 以下に説明文のコードの該当箇所を写してください\n",
    "    #-----------------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #-----------------------------------\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "print(\"\\nlabel: \", labels[0], \"\\ndocs:\\n\", docs[0])\n",
    "print(\"\\nlabel: \", labels[1000], \"\\ndocs:\\n\", docs[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 最後の`print`文では、2つの記事の全文と記事のラベル（カテゴリー）が表示されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "    \n",
    "    \n",
    "    # 例のコードを写してください。\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "print(\"\\nlabel: \", labels[0], \"\\ndocs:\\n\", docs[0])\n",
    "print(\"\\nlabel: \", labels[1000], \"\\ndocs:\\n\", docs[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "ryAXc28jUlG",
    "id": "code_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 60
   },
   "source": [
    "### 2.2.5 Word2Vec（実装）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "下準備はできたので、本題の **Word2Vec** を説明していきます。<br>\n",
    "Word2Vecを用いる時は`gensim`モジュールから`import`します。\n",
    "```python\n",
    "from gensim.models import word2vec\n",
    "```\n",
    "学習に使用するリスト（分かち書きされた文書）を **`Word2Vec`関数** の引数とすることで、モデルを生成します。<br>\n",
    "\n",
    "「BOW カウント」等で扱った`janome.tokenizer`を使って予め **分かち書き** を行います。\n",
    "分かち書きの際、各単語について品詞を調べます。  \n",
    "日本語では、「名詞、動詞、形容詞、形容動詞」以外は単語の関連性の分析に使えないので、「名詞、動詞、形容詞、形容動詞」のみの分かち書きリストを作成します。<br>\n",
    "\n",
    "\n",
    "**`Word2Vec`** は以下のようにして使います。\n",
    "```python\n",
    "model = word2vec.Word2Vec(リスト, size=a, min_count=b, window=c)\n",
    "# ただし、a, b, cは数字\n",
    "```\n",
    "`Word2Vec`のよく使う引数は主に以下です。\n",
    "> - **size** ：ベクトルの次元数。\n",
    "> - **window** ：この数の前後の単語を、関連性のある単語と見なして学習を行う。\n",
    "> - **min_count** ：n回未満登場する単語を破棄。\n",
    "\n",
    "適切に学習が行われた後、  `model`に対し\n",
    " **`.most_similar(positive=[\"単語\"])`** のように`most_similar()`メソッドを用いるとその単語との類似度が高いものが出力されます。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- `word2vec`を用いて、「男」という単語と関連性の高い単語を出力してください。`word2vec.Word2Vec`の引数に関して、リストには`[sentences]`を指定し、`size=100`, `min_count=20`, `window=15`としてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from janome.tokenizer import Tokenizer\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# livedoor newsの読み込みと分類\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "\n",
    "                #1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "# 品詞を取り出し「名詞、動詞、形容詞、形容動詞」のリスト作成\n",
    "def tokenize(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    word = []\n",
    "    for token in tokens:\n",
    "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if part_of_speech in [\"名詞\", \"動詞\", \"形容詞\", \"形容動詞\"]:\n",
    "            word.append(token.surface)            \n",
    "    return word\n",
    "\n",
    "# ラベルと文章に分類\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "sentences = tokenize(docs[0:100])  # データ量が多いため制限している\n",
    "\n",
    "# 以下に回答を作成してください\n",
    "#word2vec.Word2Vecの引数に関して、size=100, min_count=20, window=15としてください\n",
    "model = \n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "```python\n",
    "model = word2vec.Word2Vec(リスト, size=100, min_count=20, window=15)\n",
    "print(model.most_similar(positive=[\"単語\"]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from janome.tokenizer import Tokenizer\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# livedoor newsの読み込みと分類\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "\n",
    "                #1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "# 品詞を取り出し「名詞、動詞、形容詞、形容動詞」のリスト作成\n",
    "def tokenize(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    word = []\n",
    "    for token in tokens:\n",
    "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if part_of_speech in [\"名詞\", \"動詞\", \"形容詞\", \"形容動詞\"]:\n",
    "            word.append(token.surface)            \n",
    "    return word\n",
    "\n",
    "# ラベルと文章に分類\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "sentences = tokenize(docs[0:100])  # データ量が多いため制限している\n",
    "# 以下に回答を作成してください\n",
    "#word2vec.Word2Vecの引数に関して、size=100, min_count=20, window=15としてください\n",
    "model = word2vec.Word2Vec([sentences], size=100, min_count=20, window=15)\n",
    "print(model.most_similar(positive=[\"男\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "SybHdRmbeWz"
   },
   "source": [
    "## 2.3 Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "rkdsPwKlz",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.3.1 Doc2Vec（1）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>Doc2Vec</b>は、Word2Vecを応用した **文章をベクトル化する技術** です。<br>\n",
    "「文書のベクトル表現」にてBOWで文章のベクトル化を勉強しましたが、BOWとの大きな違いは **文の語順** も特徴として考慮に入れることができる点です。\n",
    "\n",
    "文書のベクトル表現にて学んだBOWの欠点をおさらいすると、以下のようになります。\n",
    "\n",
    "- 単語の語順情報がない\n",
    "- 単語の意味の表現が苦手\n",
    "\n",
    "この二点の欠点をDoc2Vecは補っています。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 選択肢から正しいものを選択してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- BOWは、単語の語順を考慮している。\n",
    "- Word2Vecは、単語の語順を考慮している。\n",
    "- Doc2Vecは、単語の意味の表現が苦手である。\n",
    "- Doc2Vecは、単語の語順を考慮している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- BOW,Word2Vecは文書のベクトル表現やWord2Vecの章で学びました。\n",
    "- Doc2VecはBOWの、単語の語順情報がない、単語の意味の表現が苦手、という点を補ったものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "Doc2Vecは、単語の語順を考慮している。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "r1xuiPDFxM",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 30
   },
   "source": [
    "### 2.3.2 Doc2Vec（2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "<b style='color: #AA0000'>Doc2Vec</b>を実装していきます。<br>\n",
    "「コーパスの取り出し」にて作成したlivedoor newsコーパスの`docs[0],docs[1],docs[2],docs[3]`の類似度を比較します。\n",
    "\n",
    "フローは以下のようになります。\n",
    "1. 分かち書き\n",
    "1. **`TaggedDocument`** クラスのインスタンスを作成\n",
    "1. **`Doc2Vec`** でモデルの生成\n",
    "1. 類似度の出力\n",
    "***\n",
    "**ポイント**\n",
    "\n",
    "1.<br>\n",
    "文章を`janome`の`Tokenizer`を用い、分かち書きにします。\n",
    "\n",
    "2.<br>\n",
    "`TaggedDocument`の引数に`words=\"分かち書きされた各要素\", tags=[\"タグ\"]`を与えると、 **TaggedDocument** クラスのインスタンスを作成できます。<br>\n",
    "タグは文書のidのようなものです。<br>\n",
    "`TaggedDocument`をリストに格納し、これをDoc2Vecに渡します。\n",
    "\n",
    "3.<br>\n",
    "モデルの学習は以下のように記述します。\n",
    "```python\n",
    "model = Doc2Vec(documents=リスト, min_count=1)\n",
    "```\n",
    "`min_count`:最低この回数出現した単語のみを学習に使用<br>\n",
    "\n",
    "4.<br>\n",
    "類似度の出力は以下のように記述します。\n",
    "```python\n",
    "for i in range(4):\n",
    "    print(model.docvecs.most_similar(\"d\"+str(i)))\n",
    "```\n",
    "\n",
    "\n",
    "問題にDoc2Vecの例を載せているので、解きながら使い方を覚えて行きましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 次のコードのモデル学習の部分を埋めて、livedoor newsコーパスの`docs[0],docs[1],docs[2],docs[3]`の類似度を比較してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# livedoor newsの読み込みと分類\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "\n",
    "                #1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# Doc2Vecの処理\n",
    "token = [] # 各docsの分かち書きした結果を格納するリストです\n",
    "training_docs = [] # TaggedDocumentを格納するリストです\n",
    "for i in range(4):\n",
    "    \n",
    "    # docs[i] を分かち書きして、tokenに格納します\n",
    "    t = Tokenizer() \n",
    "    token.append(t.tokenize(docs[i], wakati=True))\n",
    "    \n",
    "    # TaggedDocument クラスのインスタンスを作成して、結果をtraining_docsに格納します\n",
    "    # タグは \"d番号\"とします\n",
    "    training_docs.append(TaggedDocument(words=token[i], tags=[\"d\" + str(i)]))\n",
    "\n",
    "# 以下に回答を作成してください\n",
    "model = \n",
    "\n",
    "for i in range(4):\n",
    "    print(model.docvecs.most_similar(\"d\"+str(i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- `script.py`にはモデル学習の記述が抜けています。\n",
    "```python\n",
    "model = Doc2Vec(documents=リスト, min_count=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# livedoor newsの読み込みと分類\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "\n",
    "                #1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# Doc2Vecの処理\n",
    "token = [] # 各docsの分かち書きした結果を格納するリストです\n",
    "training_docs = [] # TaggedDocumentを格納するリストです\n",
    "for i in range(4):\n",
    "    \n",
    "    # docs[i] を分かち書きして、tokenに格納します\n",
    "    t = Tokenizer() \n",
    "    token.append(t.tokenize(docs[i], wakati=True))\n",
    "    \n",
    "    # TaggedDocument クラスのインスタンスを作成して、結果をtraining_docsに格納します\n",
    "    # タグは \"d番号\"とします\n",
    "    training_docs.append(TaggedDocument(words=token[i], tags=[\"d\" + str(i)]))\n",
    "\n",
    "# 以下に回答を作成してください\n",
    "model = Doc2Vec(documents=training_docs, min_count=1)\n",
    "\n",
    "for i in range(4):\n",
    "    print(model.docvecs.most_similar(\"d\"+str(i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "section_name",
    "sectionId": "HJzrOC7WlbM"
   },
   "source": [
    "## 2.4 日本語テキストの分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "Hk-uoPwYlG",
    "id": "quiz_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.4.1 日本語テキストの分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "chapter1, chapter2で学んできたことを用いて <b style='color: #AA0000'>**日本語テキストのカテゴリをランダムフォレスト**</b>で分類します。<br>\n",
    "ここでもlivedoor newsを用います。ランダムフォレストに与えるデータはベクトル表現化したニュース記事で、9種類のカテゴリーに分類します。   \n",
    "記事をベクトルで表すことにより、教師あり学習で学んだ方法をそのまま適用して記事の分類が行えます。 \n",
    "\n",
    "このチャプターの学習フローは以下のようになります。\n",
    "\n",
    "1. livedoor newsの読み込みと分類：「コーパスの取り出し」\n",
    "2. データをトレイニングデータとテストデータに分割する：機械学習概論の「ホールドアウト法の理論と実践」\n",
    "3. tf-idfでトレイニングデータとテストデータをベクトル化する：「BOW tf-idfによる重み付け（実装）」、「fit関数」\n",
    "4. ランダムフォレストで学習：教師あり分類の「ランダムフォレスト」\n",
    "5. 実装：「コーパスのカテゴリをランダムフォレストで実装」\n",
    "6. 精度を上げる：「精度をあげる」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "以下の選択肢から正しい順番に並んでいるものを選択してください。<br>\n",
    "1. 機械学習手法でデータを学習（基準の取得）\n",
    "2. データ収集\n",
    "3. テストデータで性能をテスト\n",
    "4. データクレンジング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 2  -> 3 -> 1 -> 4\n",
    "- 2  -> 1 -> 3 -> 4\n",
    "- 2  -> 3 -> 4 -> 1\n",
    "- 2 -> 4 -> 1 -> 3\n",
    "- 2 -> 1 -> 4 -> 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- データクレンジングはデータの前処理になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 2 -> 4 -> 1 -> 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "B1fusDDtef",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.4.2 fit関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "`scikit-learn` の変換系クラス(StandardScaler、Normalizer、TfidfVectorizer など)には、 **`fit()`, `fit_transform()`, `transform()`** などの関数があります。<br>\n",
    "\n",
    "**`fit()`関数** ：渡されたデータの統計（最大値、最小値、平均、など）を取得して、メモリに保存。<br>\n",
    "**`transform()`関数** ：`fit()`で取得した情報を用いてデータを書き換える。<br>\n",
    "**`fit_transform()`関数** ：`fit()`の後に`transform()`を実施する。\n",
    "\n",
    " **`fit()関数`** はトレーニングデータセットからパラメーターを学習するために使用され、 **`tranform()関数`** は　学習したパラメーターに基づいてデータが再形成されます。<br>\n",
    "つまり、トレーニングデータの場合は **`fit_transform関数`** を用い、<br>テストデータの場合は,トレーニングデータの **`fit()`** の結果に基づくので、  **`transform()関数`** を行う必要があります。\n",
    "\n",
    "<img src=\"https://aidemyexcontentspic.blob.core.windows.net/contents-pic/5050_nlp/fit_func.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下の選択肢から正しいものを選んでください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- テストデータに `fit_transform()` を用いた。\n",
    "- テストデータに `fit()` したものを `transform()` した。\n",
    "- トレーニングデータに `fit_transform()` を用いた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- fit()させるのはトレーニングデータです。テストデータにはfitさせてはいけません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- トレーニングデータに `fit_transform()` を用いた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "Bym_ovwFxG",
    "id": "code_session_name",
    "important": false,
    "isDL": false,
    "timeoutSecs": 15
   },
   "source": [
    "### 2.4.3 コーパスのカテゴリをランダムフォレストで実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "それではこれまで学んできたことを用いて、 <b style='color: #AA0000'>livedoornewsコーパスのカテゴリをランダムフォレストで分類</b>しましょう。\n",
    "\n",
    "「日本語テキストの分類」でも書いたようにフローは以下のようになります。\n",
    "\n",
    "1. livedoor newsの読み込みと分類：「コーパスの取り出し」\n",
    "2. データをトレイニングデータとテストデータに分割する：機械学習概論の「ホールドアウト法の理論と実践」\n",
    "3. tf-idfでトレイニングデータとテストデータをベクトル化する：「BOW tf-idfによる重み付け（実装）」、「fit関数」\n",
    "4. ランダムフォレストで学習：教師あり分類の「ランダムフォレスト」\n",
    "5. 実装：「コーパスのカテゴリをランダムフォレストで実装」\n",
    "6. 精度を上げる：「精度をあげる」\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- livedoornewsコーパスのカテゴリをランダムフォレストで分類しましょう。\n",
    "- `train_data`と`test_data`をtf-idfでベクトル化した値をそれぞれ`train_matrix`と`test_matrix`に代入しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "index"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "# livedoor newsの読み込みと分類(自然言語処理基礎2.2.4)\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# データをトレイニングデータとテストデータに分割(機械学習概論2.2.2)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(docs, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# tf-idfでトレイニングデータとテストデータをベクトル化(自然言語処理基礎2.1.4, 2.4.2)\n",
    "# 以下に回答を作成してください\n",
    "vectorizer = \n",
    "train_matrix = \n",
    "test_matrix = \n",
    "\n",
    "# ランダムフォレストで学習\n",
    "clf = RandomForestClassifier(n_estimators=2)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "# 精度の出力\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- トレーニングデータには`fit_transform`を、テストデータには`transform`を使用しましょう。\n",
    "- わからない部分は書いてあるセッションを参照して復習をしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "answer"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "# livedoor newsの読み込みと分類(自然言語処理基礎2.2.4)\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./5050_nlp_data/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# データをトレイニングデータとテストデータに分割(機械学習概論2.2.2)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(docs, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# tf-idfでトレイニングデータとテストデータをベクトル化(自然言語処理基礎2.1.4, 2.4.2)\n",
    "# 以下に回答を作成してください\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(train_data)\n",
    "test_matrix = vectorizer.transform(test_data)\n",
    "\n",
    "# ランダムフォレストで学習\n",
    "clf = RandomForestClassifier(n_estimators=2)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "# 精度の出力\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "courseId": 5050,
    "exerciseId": "HkEOjDDFlf",
    "id": "quiz_session_name",
    "important": true,
    "isDL": false,
    "timeoutSecs": 5
   },
   "source": [
    "### 2.4.4 精度を上げる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "description"
   },
   "source": [
    "「コーパスのカテゴリをランダムフォレストで実装」で実装したプログラムの精度を上げる作業をしたいと思います。\n",
    " **`TfidfVectorizer()`** のパラメーターに **`tokenizer=関数`** を設定すると、指定した関数でテキストを分割することができます。<br>\n",
    "例えば、以下の関数を **`tokenizer=`** の引数とすると、「名詞、動詞、形容詞、形容動詞」のみの分割されたテキストを用います。<br>\n",
    "そのため、分析に必要ない助詞・助動詞等がないので精度が上がる事になります。\n",
    "```python\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "def tokenize(text):\n",
    "    t=Tokenizer()\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    noun = []\n",
    "    for token in tokens:\n",
    "    # 品詞を取り出し\n",
    "        partOfSpeech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if partOfSpeech == \"名詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"動詞\":        \n",
    "            noun.append(token.surface)\n",
    "        if partOfSpeech == \"形容詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"形容動詞\":        \n",
    "            noun.append(token.surface)            \n",
    "    return noun\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "question"
   },
   "source": [
    "- 以下の選択肢から **最も精度が高くなると思われるもの** を選択してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "choices"
   },
   "source": [
    "- 名詞のみで分割したテキスト\n",
    "- 動詞のみで分割したテキスト\n",
    "- 形容詞のみで分割したテキスト\n",
    "- 形容動詞のみで分割したテキスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ヒント"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hint"
   },
   "source": [
    "- 最も文書の特徴がでるキーワードの品詞を考えてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 解答"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "answer"
   },
   "source": [
    "- 名詞のみで分割したテキスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}